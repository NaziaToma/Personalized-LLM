[
    {
        "Main Topic": "Array",
        "Subtopics": [
            {
                "Subtopic": "Basic terminologies of Array",
                "Description": "Array Index:  In an array, elements are identified by their indexes. Array index starts from 0. Array element:  Elements are items stored in an array and can be accessed by their index. Array Length:  The length of an array is determined by the number of elements it can contain.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Memory representation of Array",
                "Description": "In an array, all the elements are stored in contiguous memory locations. So, if we initialize an array, the elements will be allocated sequentially in memory. This allows for efficient access and manipulation of elements.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Declaration of Array",
                "Description": "Arrays can be declared in various ways in different languages. For better illustration, below are some language-specific array declarations:",
                "Code Snippets": [
                    "# In Python, all types of lists are created same way\narr = []"
                ]
            },
            {
                "Subtopic": "Initialization of Array",
                "Description": "Arrays can be initialized in different ways in different languages. Below are some language-specific array initializations:",
                "Code Snippets": [
                    "# This list will store integer type elements\narr = [1, 2, 3, 4, 5]\n\n# This list will store character type elements (strings in Python)\narr = ['a', 'b', 'c', 'd', 'e']\n\n# This list will store float type elements\narr = [1.4, 2.0, 24.0, 5.0, 0.0]  # All float values"
                ]
            },
            {
                "Subtopic": "Why do we Need Arrays?",
                "Description": "Assume there is a class of five students and if we have to keep records of their marks in examination then, we can do this by declaring five variables individual and keeping track of records but what if the number of students becomes very large, it would be challenging to manipulate and maintain the data. What it means is that, we can use normal variables (v1, v2, v3, ..) when we have a small number of objects. But if we want to store a large number of instances, it becomes difficult to manage them with normal variables.  The idea of an array is to represent many instances in one variable .",
                "Code Snippets": null
            },
            {
                "Subtopic": "Types of Arrays",
                "Description": "Arrays can be classified in two ways: On the basis of Size On the basis of Dimensions 1. Fixed Sized Arrays: We cannot alter or update the size of this array.  Here only a fixed size (i,e. the size that is mentioned in square brackets  [] ) of memory will be allocated for storage. In case, we don’t know the size of the array then if we declare a larger size and store a lesser number of elements will result in a wastage of memory or we declare a lesser size than the number of elements then we won’t get enough memory to store all the elements. In such cases, static memory allocation is not preferred.  2. Dynamic Sized Arrays: The size of the array changes as per user requirements during execution of code so the coders do not have to worry about sizes. They can add and removed the elements as per the need. The memory is mostly dynamically allocated and de-allocated in these arrays. 1. One-dimensional Array(1-D Array):  You can imagine a 1d array as a row, where elements are stored one after another.  2. Multi-dimensional Array:  A multi-dimensional array is an array with more than one dimension. We can use multidimensional array to store complex data in the form of tables, etc. We can have 2-D arrays, 3-D arrays, 4-D arrays and so on. Two-Dimensional Array(2-D Array or Matrix):   2-D Multidimensional arrays can be considered as an array of arrays or as a matrix consisting of rows and columns.  Three-Dimensional Array(3-D Array):  A 3-D Multidimensional array contains three dimensions, so it can be considered an array of two-dimensional arrays.",
                "Code Snippets": [
                    "# Create a fixed-size list of length 5, \n# initialized with zeros\narr = [0] * 5\n\n# Output the fixed-size list\nprint(arr)\n# Dynamic Array\narr = []"
                ]
            },
            {
                "Subtopic": "Operations on Array",
                "Description": "Array traversal involves visiting all the elements of the array once. Below is the implementation of Array traversal in different Languages: We can insert one or multiple elements at any position in the array. Below is the implementation of Insertion in Array in different languages: We can delete an element at any index in an array. Below is the implementation of Deletion of element in an array: We can traverse over an array and search for an element. Below is the implementation of Searching of element in an array:",
                "Code Snippets": [
                    "# This list will store integer type elements\narr = [1, 2, 3, 4, 5]\n\n# Traversing over arr\nfor i in range(len(arr)):\n    print(arr[i], end=\" \")\n# Example usage\narr = [1, 2, 3, 4, 5]\nx = 10  # Element to be inserted\npos = 2  # Position to insert the element\n\narr.insert(pos, x)\n\n# Print the updated list\nprint(\"Updated List:\", arr)\n# Initialize a list\narr = [10, 20, 30, 40, 50]\n\n# Value to delete\nkey = 40\n\n# Remove the element with the specified value\n# if present in the list\nif key in arr:\n   arr.remove(key)\nelse:\n   print(\"Element Not Found\")\n\n# Output the modified list\nprint(arr)  # Output: [10, 20, 30, 50]\n# Function to implement search operation\ndef find_element(arr, n, key):\n    for i in range(n):\n        if arr[i] == key:\n            return i\n    return -1"
                ]
            }
        ]
    },
    {
        "Main Topic": "Hashing",
        "Subtopics": [
            {
                "Subtopic": "Hash Data Structure Overview",
                "Description": "It is one of the most widely used data structure after arrays.  \n It mainly supports search, insert and delete in O(1) time on average which is more efficient than other popular data structures like arrays, Linked List and  Self Balancing BST . \n We use hashing for dictionaries, frequency counting, maintaining data for quick access by key, etc.  \n Real World Applications include Database Indexing, Cryptography, Caches, Symbol Table and Dictionaries. \n There are mainly two forms of hash typically implemented in programming languages.  Hash Set  :  Collection of unique keys (Implemented as  Set in Python ,  Set in JavaScrtipt ,  unordered_set in C++  and  HashSet in Java . Hash Map  : Collection of key value pairs with keys being unique (Implemented as  dictionary in Python,   Map in JavaScript ,  unordered_map in C++  and  HashMap in Java ) Need to maintain sorted data along with search, insert and delete. We use a self balancing BST in these cases. \n When Strings are keys and we need operations like prefix search along with search, insert and delete. We use Trie in these cases. \n When we need operations like floor and ceiling along with search, insert and/or delete. We use Self Balancing BST in these cases.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Components of Hashing",
                "Description": "There are majorly three components of hashing:\n\nKey: A Key can be anything string or integer which is fed as input in the hash function the technique that determines an index or location for storage of an item in a data structure.\nHash Function: Receives the input key and returns the index of an element in an array called a hash table. The index is known as the hash index .\nHash Table: Hash table is typically an array of lists. It stores values corresponding to the keys. Hash stores the data in an associative manner in an array where each data value has its own unique index",
                "Code Snippets": null
            },
            {
                "Subtopic": "How does Hashing work?",
                "Description": "Suppose we have a set of strings {“ab”, “cd”, “efg”} and we would like to store it in a table. Step 1:  We know that hash functions (which is some mathematical formula) are used to calculate the hash value which acts as the index of the data structure where the value will be stored.  \n Step 2:  So, let’s assign  \n \n  “a” = 1,  \n  “b”=2, .. etc, to all alphabetical characters.  \n \n \n Step 3:  Therefore, the numerical value by summation of all characters of the string: Step 4:  Now, assume that we have a table of size 7 to store these strings. The hash function that is used here is the sum of the characters in   key mod Table size   . We can compute the location of the string in the array by taking the   sum(string) mod 7   .  \n Step 5:   So we will then store  \n \n  “ab” in 3 mod 7 = 3,  \n  “cd” in 7 mod 7 = 0, and  \n  “efg” in 18 mod 7 = 4.  The above technique enables us to calculate the location of a given string by using a simple hash function and rapidly find the value that is stored in that location. Therefore the idea of hashing seems like a great way to store (key, value) pairs of the data in a table.",
                "Code Snippets": null
            },
            {
                "Subtopic": "What is a Hash function?",
                "Description": "A   hash function   creates a mapping from an input key to an index in hash table, this is done through the use of mathematical formulas known as hash functions. For example:  Consider phone numbers as keys and a hash table of size 100. A simple example hash function can be to consider the last two digits of phone numbers so that we have valid array indexes as output. There are many hash functions that use numeric or alphanumeric keys. This article focuses on discussing   different hash functions   : A good hash function should have the following properties:",
                "Code Snippets": null
            },
            {
                "Subtopic": "What is Collision?",
                "Description": "If we consider the above example, the hash function we used is the sum of the letters, but if we examined the hash function closely then the problem can be easily visualised that for different strings same hash value is being generated by the hash function. For example: {“ab”, “ba”} both have the same hash value, and string {“cd”,”be”} also generate the same hash value, etc. This is known as   collision   and it creates problem in searching, insertion, deletion, and updating of value. The probability of a hash collision depends on the size of the algorithm, the distribution of hash values and the efficiency of Hash function.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How to handle Collisions?",
                "Description": "There are mainly two methods to handle collision:  Refer this to read in detail:  Collision Resolution Techniques . The   load factor   of the hash table can be defined as the number of items the hash table contains divided by the size of the hash table. Load factor is the decisive parameter that is used when we want to rehash the previous hash function or want to add more elements to the existing hash table. It helps us in determining the efficiency of the hash function i.e. it tells whether the hash function which we are using is distributing the keys uniformly or not in the hash table.",
                "Code Snippets": null
            },
            {
                "Subtopic": "What is Rehashing?",
                "Description": "As the name suggests,   rehashing   means hashing again. Basically, when the load factor increases to more than its predefined value (the default value of the load factor is 0.75), the complexity increases. So to overcome this, the size of the array is increased (doubled) and all the values are hashed again and stored in the new double-sized array to maintain a low load factor and low complexity.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Linked List",
        "Subtopics": [
            {
                "Subtopic": "Understanding Node Structure",
                "Description": "In a singly linked list, each node consists of two parts: data and a pointer to the next node. This structure allows nodes to be dynamically linked together, forming a chain-like sequence. In this example, the Node class contains an integer data field ( data ) to store the information and a pointer to another Node ( next ) to establish the link to the next node in the list.",
                "Code Snippets": [
                    "# Definition of a Node in a singly linked list\nclass Node:\n    def __init__(self, data):\n       # Data part of the node\n        self.data = data   \n        self.next = None"
                ]
            },
            {
                "Subtopic": "Operations on Singly Linked List",
                "Description": "Traversal Searching Length Insertion: Insert at the beginning Insert at the end Insert at a specific position Deletion: Delete from the beginning Delete from the end Delete a specific node Let's go through each of the operations mentioned above, one by one.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Traversal of Singly Linked List",
                "Description": "Traversal involves visiting each node in the linked list and performing some operation on the data. A simple traversal function would print or process the data of each node. Step-by-step approach: Initialize a pointer current to the head of the list. Use a while loop to iterate through the list until the current pointer reaches NULL. Inside the loop, print the data of the current node and move the current pointer to the next node. Below is the function for traversal in singly Linked List:",
                "Code Snippets": [
                    "# Python Function to traverse and print the elements of the linked list\ndef traverse_linked_list(head):\n    # Start from the head of the linked list\n    current = head\n\n    # Traverse the linked list until reaching the end (None)\n    while current is not None:\n\n        # Print the data of the current node followed by a space\n        print(current.data),\n\n        # Move to the next node\n        current = current.next\n\n    print()  # Print a new line after traversing the linked list"
                ]
            },
            {
                "Subtopic": "Searching in Singly Linked List",
                "Description": "Searching in a Singly Linked List refers to the process of looking for a specific element or value within the elements of the linked list. Step-by-step approach: Below is the function for searching in singly linked list:",
                "Code Snippets": [
                    "# Python function to search for a value in the Linked List\ndef search_linked_list(head, target):\n\n    # Traverse the Linked List\n    while head is not None:\n\n        # Check if the current node's data matches the target value\n        if head.data == target:\n\n            return True  # Value found\n        # Move to the next node\n        head = head.next\n\n    return False  # Value not found"
                ]
            },
            {
                "Subtopic": "Length of Singly Linked List",
                "Description": "Finding Length in Singly Linked List refers to the process of determining the total number of nodes in a singly linked list. Step-by-step approach: Initialize a counter  length  to 0. Start from the head of the list, assign it to current. Traverse the list: Increment  length  for each node. Move to the next node ( current = current->next ). Return the final value of  length . Below is the function for finding length in Singly Linked List:",
                "Code Snippets": [
                    "# Python function to find the length of the linked list\ndef find_length(head):\n  \n    # Initialize a counter for the length\n    length = 0\n\n    # Start from the head of the list\n    current = head\n\n    # Traverse the list and increment the length for each\n    # node\n    while current is not None:\n        length += 1\n        current = current.next\n\n    # Return the final length of the linked list\n    return length"
                ]
            },
            {
                "Subtopic": "Insertion in Singly Linked List",
                "Description": "Insertion is a fundamental operation in linked lists that involves adding a new node to the list. There are several scenarios for insertion: Step-by-step approach: Create a new node with the given value. Set the  next  pointer of the new node to the current head. Move the head to point to the new node. Return the new head of the linked list. Below is the function for insertion at the beginning of singly linked list: To insert a node at the end of the list, traverse the list until the last node is reached, and then link the new node to the current last node- Step-by-step approach: Create a new node with the given value. Check if the list is empty: If it is, make the new node the head and return. Traverse the list until the last node is reached. Link the new node to the current last node by setting the last node's next pointer to the new node. Below is the function for insertion at the end of singly linked list: To insert a node at a specific position, traverse the list to the desired position, link the new node to the next node, and update the links accordingly. We mainly find the node after which we need to insert the new node. If we encounter a NULL before reaching that node, it means that the given position is invalid. Below is the function for insertion at a specific position of the singly linked list:",
                "Code Snippets": [
                    "# Python function to insert a new node at the beginning of the\n# linked list\ndef insert_at_beginning(head, value):\n  \n    # Create a new node with the given value\n    new_node = Node(value)\n\n    # Set the next pointer of the new node to the current\n    # head\n    new_node.next = head\n\n    # Move the head to point to the new node\n    head = new_node\n\n    # Return the new head of the linked list\n    return head\n# Python function to insert a node at the end of the linked\n# list\ndef insert_at_end(head, value):\n  \n    # Create a new node with the given value\n    new_node = Node(value)\n\n    # If the list is empty, make the new node the head\n    if head is None:\n        return new_node\n\n    # Traverse the list until the last node is reached\n    current = head\n    while current.next is not None:\n        current = current.next\n\n    # Link the new node to the current last node\n    current.next = new_node\n\n    return head\n# Function to insert a node at a specified position\ndef insertPos(head, pos, data):\n    if pos < 1:\n        print(\"Invalid position!\")\n        return head\n\n    # Special case for inserting at the head\n    if pos == 1:\n        new_node = Node(data)\n        new_node.next = head\n        return new_node\n\n    # Traverse the list to find the node before \n    # the insertion point\n    prev = head\n    count = 1\n    while count < pos - 1 and prev is not None:\n        prev = prev.next\n        count += 1\n\n    # If position is greater than the number of nodes\n    if prev is None:\n        print(\"Invalid position!\")\n        return head\n\n    # Insert the new node at the specified position\n    new_node = Node(data)\n    new_node.next = prev.next\n    prev.next = new_node\n\n    return head"
                ]
            },
            {
                "Subtopic": "Deletion in Singly Linked List",
                "Description": "Deletion involves removing a node from the linked list. Similar to insertion, there are different scenarios for deletion: To delete the first node, update the head to point to the second node in the list. Steps-by-step approach: Check if the head is  NULL . If it is, return  NULL  (the list is empty). Store the current head node in a temporary variable  temp . Move the head pointer to the next node. Delete the temporary node. Return the new head of the linked list. Below is the function for deletion at the beginning of singly linked list: To delete the last node, traverse the list until the second-to-last node and update its next field to None. Step-by-step approach: Check if the head is  NULL . If it is, return NULL (the list is empty). Check if the head's  next  is  NULL  (only one node in the list). If true, delete the head and return  NULL . Traverse the list to find the second last node ( second_last ). Delete the last node (the node after  second_last ). Set the  next  pointer of the second last node to  NULL . Return the head of the linked list. Below is the function for deletion at the end of singly linked list: To delete a node at a specific position, traverse the list to the desired position, update the links to bypass the node to be deleted. Step-by-step approach: Check if the list is empty or the position is invalid, return if so. If the head needs to be deleted, update the head and delete the node. Traverse to the node before the position to be deleted. If the position is out of range, return. Store the node to be deleted. Update the links to bypass the node. Delete the stored node. Below is the function for deletion at a specific position of singly linked list:",
                "Code Snippets": [
                    "# Python Function to remove the first node\n# of the linked list\ndef removeFirstNode(head):\n    if not head:\n        return None\n    temp = head\n\n    # Move the head pointer to the next node\n    head = head.next\n    temp = None\n    return head\n# Python Function to remove the last node of the linked list\ndef removeLastNode(head):\n    # If the list is empty, return None\n    if head is None:\n        return None\n\n    # If the list has only one node, delete it and return None\n    if head.next is None:\n        head = None\n        return None\n\n    # Find the second last node\n    second_last = head\n    while second_last.next.next is not None:\n        second_last = second_last.next\n\n    # Remove the last node\n    second_last.next = None\n\n    # Return the modified list\n    return head\n# Python function to delete a node at a specific position\ndef delete_at_position(head, position):\n    # If the list is empty or the position is invalid\n    if head is None or position < 1:\n        return head\n\n    # If the head needs to be deleted\n    if position == 1:\n        temp = head\n        head = head.next\n        temp = None\n        return head\n\n    # Traverse to the node before the position to be deleted\n    current = head\n    for i in range(1, position - 1):\n        if current is not None:\n            current = current.next\n\n    # If the position is out of range\n    if current is None or current.next is None:\n        return head\n\n    # Store the node to be deleted\n    temp = current.next\n\n    # Update the links to bypass the node to be deleted\n    current.next = current.next.next\n\n    # Delete the node\n    temp = None\n    return head"
                ]
            },
            {
                "Subtopic": "What is a Doubly Linked List?",
                "Description": "A  doubly linked list  is a data structure that consists of a set of nodes, each of which contains a  value  and  two pointers , one pointing to the  previous node  in the list and one pointing to the  next node  in the list. This allows for efficient traversal of the list in  both directions , making it suitable for applications where frequent  insertions  and  deletions  are required.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Representation of Doubly Linked List in Data Structure",
                "Description": "In a data structure, a doubly linked list is represented using nodes that have three fields:",
                "Code Snippets": null
            },
            {
                "Subtopic": "Node Definition",
                "Description": "Here is how a node in a Doubly Linked List is typically represented: Each node in a  Doubly Linked List  contains the  data  it holds, a pointer to the  next  node in the list, and a pointer to the  previous  node in the list. By linking these nodes together through the  next  and  prev  pointers, we can traverse the list in both directions (forward and backward), which is a key feature of a Doubly Linked List.",
                "Code Snippets": [
                    "class Node:\n  \n    def __init__(self, data):\n        # To store the value or data.\n        self.data = data\n\n        # Reference to the previous node\n        self.prev = None\n\n        # Reference to the next node\n        self.next = None"
                ]
            },
            {
                "Subtopic": "Operations on Doubly Linked List",
                "Description": "Traversal in Doubly Linked List Searching in Doubly Linked List Finding Length of Doubly Linked List Insertion in Doubly Linked List : Insertion at the beginning of Doubly Linked List Insertion at the end of the Doubly Linked List Insertion at a specific position in Doubly Linked List Deletion in Doubly Linked List : Deletion of a node at the beginning of Doubly Linked List Deletion of a node at the end of Doubly Linked List Deletion of a node at a specific position in Doubly Linked List Let's go through each of the operations mentioned above, one by one.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Traversal in Doubly Linked List",
                "Description": "To Traverse the doubly list, we can use the following steps: a. Forward Traversal: Initialize a pointer to the head of the linked list. While the pointer is not null: Visit the data at the current node. Move the pointer to the next node. b. Backward Traversal: Initialize a pointer to the tail of the linked list. While the pointer is not null: Visit the data at the current node. Move the pointer to the previous node. Below are the implementation of the above approach:",
                "Code Snippets": [
                    "# Define the Node class\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n# Function to traverse the doubly linked list \n# in forward direction\ndef forward_traversal(head):\n  \n    # Start traversal from the head of the list\n    curr = head\n    \n    # Continue until the current node is \n    # null (end of the list)\n    while curr is not None:\n      \n        # Output data of the current node\n        print(curr.data, end=\" \")\n        \n        # Move to the next node\n        curr = curr.next\n        \n    # Print newline after traversal\n    print()\n\n# Function to traverse the doubly linked \n# list in backward direction\ndef backward_traversal(tail):\n  \n    # Start traversal from the tail of the list\n    curr = tail\n    \n    # Continue until the current node \n    # is null (end of the list)\n    while curr is not None:\n      \n        # Output data of the current node\n        print(curr.data, end=\" \")\n        \n        # Move to the previous node\n        curr = curr.prev\n        \n    # Print newline after traversal\n    print()\n\n# Sample usage of the doubly linked list \n# and traversal functions\nif __name__ == \"__main__\":\n  \n    # Create a doubly linked list with 3 nodes\n    head = Node(1)\n    second = Node(2)\n    third = Node(3)\n\n    head.next = second\n    second.prev = head\n    second.next = third\n    third.prev = second\n\n    print(\"Forward Traversal:\")\n    forward_traversal(head)\n\n    print(\"Backward Traversal:\")\n    backward_traversal(third)"
                ]
            },
            {
                "Subtopic": "Finding Length of Doubly Linked List",
                "Description": "To find the length of doubly list, we can use the following steps: Start at the head of the list. Traverse through the list, counting each node visited. Return the total count of nodes as the length of the list. Below are the implementation of the above approach:",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, val):\n        self.data = val\n        self.prev = None\n        self.next = None\n\n# Function to find the length of \n# a doubly linked list\ndef find_length(head):\n    count = 0\n    cur = head\n    while cur is not None:\n        count += 1\n        cur = cur.next\n    return count\n\n# Driver code\nif __name__ == \"__main__\":\n  \n    # Create a doubly linked list \n    # with 3 nodes\n    head = Node(1)\n    second = Node(2)\n    third = Node(3)\n\n    head.next = second\n    second.prev = head\n    second.next = third\n    third.prev = second\n\n    print(\"Length of the doubly linked list: \" +\n          str(find_length(head)))"
                ]
            },
            {
                "Subtopic": "Insertion at the Beginning in Doubly Linked List",
                "Description": "To insert a new node at the beginning of the doubly list, we can use the following steps: Create a new node, say  new_node  with the given data and set its previous pointer to null,  new_node->prev =   NULL . Set the next pointer of new_node to current head,  new_node->next = head. If the linked list is not empty, update the previous pointer of the current head to new_node,  head->prev = new_node . Return new_node as the head of the updated linked list. Below are the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to insert a node at the beginning \n#of doubly linked list\n\n# Node structure for the doubly linked list\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n# Insert a node at the beginning\ndef insertBegin(head, data):\n    \n    # Create a new node\n    new_node = Node(data)\n    \n    # Make next of it as head\n    new_node.next = head\n    \n    # Set previous of head as new node\n    if head is not None:\n        head.prev = new_node\n    \n    # Return new node as new head\n    return new_node\n\n# Print the doubly linked list\ndef printList(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n\nif __name__ == \"__main__\":\n  \n    # Create a hardcoded doubly linked list:\n    # 2 <-> 3 <-> 4\n    head = Node(2)\n    head.next = Node(3)\n    head.next.prev = head\n    head.next.next = Node(4)\n    head.next.next.prev = head.next\n\n    # Print the original list\n    print(\"Original Linked List:\", end=' ')\n    printList(head)\n\n    # Insert a new node at the front of the list\n    head = insertBegin(head, 1)\n\n    # Print the updated list\n    print(\"After inserting Node 1 at the front:\", end=' ')\n    printList(head)"
                ]
            },
            {
                "Subtopic": "Insertion at the End of Doubly Linked List",
                "Description": "To insert a new node at the end of the doubly linked list, we can use the following steps: Allocate memory for a new node and assign the provided value to its data field. Initialize the next pointer of the new node to nullptr. If the list is empty: Set the previous pointer of the new node to nullptr. Update the head pointer to point to the new node. If the list is not empty: Traverse the list starting from the head to reach the last node. Set the next pointer of the last node to point to the new node. Set the previous pointer of the new node to point to the last node. Below are the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to insert a node at the end of \n#doubly linked list\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n        self.prev = None\n\n# Function to insert a new node at the end of the \n#doubly linked list\ndef insert_end(head, new_data):\n      \n    # Create a new node\n    new_node = Node(new_data)\n    \n    # If the linked list is empty, set the new node\n    #as the head\n    if head is None:\n        head = new_node\n    else:\n        curr = head\n        while curr.next is not None:\n            curr = curr.next\n        \n        # Set the next of the last node to the new node\n        curr.next = new_node\n        \n        # Set the prev of the new node to the last node\n        new_node.prev = curr\n    \n    return head\n\ndef print_list(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n\nif __name__ == \"__main__\":\n  \n    # Create a hardcoded doubly linked list:\n    # 1 <-> 2 <-> 3\n    head = Node(1)\n    head.next = Node(2)\n    head.next.prev = head\n    head.next.next = Node(3)\n    head.next.next.prev = head.next\n\n    # Print the original list\n    print(\"Original Linked List: \", end=\"\")\n    print_list(head)\n\n    # Insert a new node with data 4 at the end\n    print(\"Inserting Node with data 4 at the end: \", end=\"\")\n    data = 4\n    head = insert_end(head, data)\n\n    # Print the updated list\n    print_list(head)"
                ]
            },
            {
                "Subtopic": "Insertion at a Specific Position in Doubly Linked List",
                "Description": "To insert a node at a specific Position in doubly linked list, we can use the following steps: To insert a new node at a specific position, If position = 1, create a new node and make it the head of the linked list and return it. Otherwise, traverse the list to reach the node at position – 1, say  curr . If the position is valid, create a new node with given data, say  new_node . Update the next pointer of new node to the next of current node and prev pointer of new node to current node,  new_node->next = curr->next  and  new_node->prev = curr. Similarly, update next pointer of current node to the   new node,  curr->next = new_node . If the new node is not the last node, update prev pointer of new node’s next to the new node,  new_node->next->prev = new_node. Below is the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to insert a node at a given position\n\nclass Node:\n    def __init__(self, new_data):\n        self.data = new_data\n        self.next = None\n        self.prev = None\n\ndef insert_at_position(head, pos, new_data):\n  \n    # Create a new node\n    new_node = Node(new_data)\n\n    # Insertion at the beginning\n    if pos == 1:\n        new_node.next = head\n\n        # If the linked list is not empty, set the\n        #prev of head to new node\n        if head is not None:\n            head.prev = new_node\n\n        # Set the new node as the head of the linked list\n        head = new_node\n        return head\n\n    curr = head\n    \n    # Traverse the list to find the node before the \n    #insertion point\n    for _ in range(1, pos - 1):\n        if curr is None:\n            print(\"Position is out of bounds.\")\n            return head\n        curr = curr.next\n\n    # If the position is out of bounds\n    if curr is None:\n        print(\"Position is out of bounds.\")\n        return head\n\n    # Set the prev of new node to curr\n    new_node.prev = curr\n\n    # Set the next of new node to next of curr\n    new_node.next = curr.next\n\n    # Update the next of current node to new node\n    curr.next = new_node\n\n    # If the new node is not the last node, update \n    #prev of next node to new node\n    if new_node.next is not None:\n        new_node.next.prev = new_node\n\n    return head\n\ndef print_list(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n\nif __name__ == \"__main__\":\n  \n    # Create a hardcoded doubly linked list:\n    # 1 <-> 2 <-> 4\n    head = Node(1)\n    head.next = Node(2)\n    head.next.prev = head\n    head.next.next = Node(4)\n    head.next.next.prev = head.next\n\n    # Print the original list\n    print(\"Original Linked List: \", end=\"\")\n    print_list(head)\n\n    # Insert new node with data 3 at position 3\n    print(\"Inserting Node with data 3 at position 3: \", end=\"\")\n    data = 3\n    pos = 3\n    head = insert_at_position(head, pos, data)\n\n    # Print the updated list\n    print_list(head)"
                ]
            },
            {
                "Subtopic": "Deletion at the Beginning of Doubly Linked List",
                "Description": "To delete a node at the beginning in doubly linked list, we can use the following steps: Check if the list is empty, there is nothing to delete. Return. Store the head pointer in a variable, say  temp . Update the head of linked list to the node next to the current head,  head = head->next . If the new head is not NULL, update the previous pointer of new head to NULL,  head->prev = NULL . Below is the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to delete a node from the \n# beginning of Doubly Linked List\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n# Function to delete the first node (head) of the list\n# and return the second node as the new head\ndef del_head(head):\n  \n    # If empty, return None\n    if head is None:\n        return None\n\n    # Store in temp for deletion later\n    temp = head\n\n    # Move head to the next node\n    head = head.next\n\n    # Set prev of the new head\n    if head is not None:\n        head.prev = None\n\n    # Return new head\n    return head\n\ndef print_list(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n    \n\nif __name__ == \"__main__\":\n  \n\t# Create a hardcoded doubly linked list:\n    # 1 <-> 2 <-> 3\n    head = Node(1)\n    head.next = Node(2)\n    head.next.prev = head\n    head.next.next = Node(3)\n    head.next.next.prev = head.next\n\n    print(\"Original Linked List: \", end=\"\")\n    print_list(head)\n\n    print(\"After Deletion at the beginning: \", end=\"\")\n    head = del_head(head)\n\n    print_list(head)"
                ]
            },
            {
                "Subtopic": "Deletion at the End of Doubly Linked List",
                "Description": "To delete a node at the end in doubly linked list, we can use the following steps: Check if the doubly linked list is empty. If it is empty, then there is nothing to delete. If the list is not empty, then move to the last node of the doubly linked list, say  curr . Update the second-to-last node's next pointer to NULL,  curr->prev->next = NULL . Free the memory allocated for the node that was deleted. Below is the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to delete a node from the end of \n#Doubly Linked List\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\ndef del_last(head):\n  \n    # Corner cases\n    if head is None:\n        return None\n    if head.next is None:\n        return None\n\n    # Traverse to the last node\n    curr = head\n    while curr.next is not None:\n        curr = curr.next\n\n    # Update the previous node's next pointer\n    if curr.prev is not None:\n        curr.prev.next = None\n\n    # Return the updated head\n    return head\n\ndef print_list(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n\nif __name__ == \"__main__\":\n  \n    # Create a hardcoded doubly linked list:\n    # 1 <-> 2 <-> 3\n    head = Node(1)\n    head.next = Node(2)\n    head.next.prev = head\n    head.next.next = Node(3)\n    head.next.next.prev = head.next\n\n    print(\"Original Linked List: \", end=\"\")\n    print_list(head)\n\n    print(\"After Deletion at the end: \", end=\"\")\n    head = del_last(head)\n\n    print_list(head)"
                ]
            },
            {
                "Subtopic": "Deletion at a Specific Position in Doubly Linked List",
                "Description": "To delete a node at a specific position in doubly linked list, we can use the following steps: Traverse to the node at the specified position, say  curr . If the position is valid, adjust the pointers to skip the node to be deleted. If curr is not the head of the linked list, update the next pointer of the node before curr to point to the node after curr,  curr->prev->next = curr-next . If curr is not the last node of the linked list, update the previous pointer of the node after curr to the node before curr,  curr->next->prev = curr->prev . Free the memory allocated for the deleted node. Below is the implementation of the above approach:",
                "Code Snippets": [
                    "# Python Program to delete node at a specific position\n#in Doubly Linked List\n\n\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.prev = None\n        self.next = None\n\n\n# Function to delete a node at a specific position \n#in the doubly linked list\ndef del_pos(head, pos):\n    # If the list is empty\n    if head is None:\n        return head\n\n    curr = head\n\n    # Traverse to the node at the given position\n    for i in range(1, pos):\n        if curr is None:\n            return head\n        curr = curr.next\n\n    # If the position is out of range\n    if curr is None:\n        return head\n\n    # Update the previous node's next pointer\n    if curr.prev is not None:\n        curr.prev.next = curr.next\n\n    # Update the next node's prev pointer\n    if curr.next is not None:\n        curr.next.prev = curr.prev\n\n    # If the node to be deleted is the head node\n    if head == curr:\n        head = curr.next\n\n    # Return the updated head\n    return head\n\n\ndef print_list(head):\n    curr = head\n    while curr is not None:\n        print(curr.data, end=\" \")\n        curr = curr.next\n    print()\n\n\nif __name__ == \"__main__\":\n  \n    # Create a hardcoded doubly linked list:\n    # 1 <-> 2 <-> 3\n    head = Node(1)\n    head.next = Node(2)\n    head.next.prev = head\n    head.next.next = Node(3)\n    head.next.next.prev = head.next\n\n    print(\"Original Linked List: \", end=\"\")\n    print_list(head)\n\n    print(\"After Deletion at the position 2: \", end=\"\")\n    head = del_pos(head, 2)\n\n    print_list(head)"
                ]
            },
            {
                "Subtopic": "Advantages of Doubly Linked List",
                "Description": "Efficient traversal in both directions:  Doubly linked lists allow for efficient traversal of the list in both directions, making it suitable for applications where frequent insertions and deletions are required. Easy insertion and deletion of nodes:  The presence of pointers to both the previous and next nodes makes it easy to insert or delete nodes from the list, without having to traverse the entire list. Can be used to implement a stack or queue:  Doubly linked lists can be used to implement both stacks and queues, which are common data structures used in programming.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Doubly Linked List",
                "Description": "More complex than singly linked lists:  Doubly linked lists are more complex than singly linked lists, as they require additional pointers for each node. More memory overhead:  Doubly linked lists require more memory overhead than singly linked lists, as each node stores two pointers instead of one.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Doubly Linked List",
                "Description": "Implementation of undo and redo functionality in text editors. Cache implementation where quick insertion and deletion of elements are required. Browser history management to navigate back and forth between visited pages. Music player applications to manage playlists and navigate through songs efficiently. Implementing data structures like  Deque  (double-ended queue) for efficient insertion and deletion at both ends. Practice Questions on Doubly Linked List MCQs on Linked List",
                "Code Snippets": null
            },
            {
                "Subtopic": "What is a Circular Linked List?",
                "Description": "A  circular linked list  is a special type of linked list where all the nodes are connected to form a circle. Unlike a regular linked list, which ends with a node pointing to  NULL , the last node in a circular linked list points back to the first node. This means that you can keep traversing the list without ever reaching a  NULL  value.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Types of Circular Linked Lists",
                "Description": "We can create a circular linked list from both  singly linked lists  and  doubly linked lists . So, circular linked list are basically of two types: In  Circular Singly Linked List , each node has just one pointer called the “ next ” pointer. The next pointer of  last node  points back to the  first node  and this results in forming a circle. In this type of Linked list we can only move through the list in one direction. In  circular doubly linked   list,  each node has two pointers  prev  and  next,  similar to doubly linked list. The  prev  pointer points to the previous node and the  next  points to the next node. Here, in addition to the  last  node storing the address of the first node, the  first node  will also store the address of the  last node . Note:  In this article, we will use the circular singly linked list to explain the working of circular linked lists.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Representation of a Circular Singly Linked List",
                "Description": "Let’s take a look on the structure of a circular linked list. Syntax to Declare a Circular Linked List in Different Languages: In the code above, each node has  data  and a  pointer  to the next node. When we create multiple nodes for a circular linked list, we only need to connect the last node back to the first one.",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None"
                ]
            },
            {
                "Subtopic": "Example of Creating a Circular Linked List",
                "Description": "Here’s an example of creating a circular linked list with three nodes (2, 3, 4): In the above code, we have created three nodes  first, second,  and  last  having values  2, 3,  and  4  respectively. After creating three nodes, we have connected these node in a series. Connect the first node “ first”  to “ second”  node by  s toring the address of “ second”  node   into  first’s  next Connect the second node “ second”  to “ second”  node by  s toring the address of “ third ” node into  second’s  next After connecting all the nodes, we reach the key characteristic of a circular linked list:  linking the last node back to the first node . Therefore, we store the address of the “ first ” node in the “ last ” node. For the insertion of a node at the beginning, we need to traverse the whole list. Also, for insertion at the end, the whole list has to be traversed. If instead of the start pointer, we take a pointer to the last node, then in both cases there won’t be any need to traverse the whole list. So insertion at the beginning or at the end takes constant time, irrespective of the length of the list.",
                "Code Snippets": [
                    "# Initilize and allocate memory for nodes\nfirst = Node(2)\nsecond = Node(3)\nlast = Node(4)\n\n# Connect nodes\nfirst.next = second\nsecond.next = last\nlast.next = first"
                ]
            },
            {
                "Subtopic": "Operations on the Circular Linked list:",
                "Description": "We can do some operations on the circular linked list similar to the singly and doubly linked list which are: Insertion Insertion at the empty list Insertion at the beginning Insertion at the end  Insertion at the given position Deletion Delete the first node Delete the last node Delete the node from any position Searching Note:  We will be using the circular singly linked list to represent the working of the circular linked list.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Insertion in the circular linked list:",
                "Description": "Insertion is a fundamental operation in linked lists that involves adding a new node to the list. The only extra step is connecting the last node to the first one. In the circular linked list mentioned below, we can insert nodes in four ways:",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, value):\n        self.data = value\n        self.next = self  # Point to itself\n\ndef insertInEmptyList(last, data):\n    if last is not None:\n        return last\n    \n    # Create a new node\n    new_node = Node(data)\n    \n    # Update last to point to the new node\n    last = new_node\n    return last\n\ndef printList(last):\n    if last is None:\n        return\n    \n    # Start from the head node\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\nif __name__ == \"__main__\":\n    last = None\n\n    # Insert a node into the empty list\n    last = insertInEmptyList(last, 1)\n\n    # Print the list\n    print(\"List after insertion: \", end=\"\")\n    printList(last)\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\n# Function to insert a node at the beginning of the circular linked list\ndef insert_at_beginning(last, value):\n    new_node = Node(value)\n\n    # If the list is empty, make the new node point to itself and set it as last\n    if last is None:\n        new_node.next = new_node\n        return new_node\n\n    # Insert the new node at the beginning\n    new_node.next = last.next\n    last.next = new_node\n\n    return last\n\n# Function to print the circular linked list\ndef print_list(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\n# Create circular linked list: 2, 3, 4\nfirst = Node(2)\nfirst.next = Node(3)\nfirst.next.next = Node(4)\nlast = first.next.next\nlast.next = first\n\nprint(\"Original list: \", end=\"\")\nprint_list(last)\n\n# Insert 5 at the beginning\nlast = insert_at_beginning(last, 5)\n\nprint(\"List after inserting 5 at the beginning: \", end=\"\")\nprint_list(last)\nclass Node:\n    def __init__(self, value):\n        self.data = value\n        self.next = None\n\n# Function to insert a node at the end of a circular linked list\n\n\ndef insert_end(tail, value):\n    new_node = Node(value)\n    if tail is None:\n        # If the list is empty, initialize\n        # it with the new node\n        tail = new_node\n        new_node.next = new_node\n    else:\n        # Insert new node after the current tail\n        # and update the tail pointer\n        new_node.next = tail.next\n        tail.next = new_node\n        tail = new_node\n    return tail\n\n# Function to print the circular linked list\n\n\ndef print_list(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\n\nif __name__ == \"__main__\":\n    # Create circular linked list: 2, 3, 4\n    first = Node(2)\n    first.next = Node(3)\n    first.next.next = Node(4)\n\n    last = first.next.next\n    last.next = first\n\n    print(\"Original list: \", end=\"\")\n    print_list(last)\n\n    # Insert elements at the end of the circular linked list\n    last = insert_end(last, 5)\n    last = insert_end(last, 6)\n\n    print(\"List after inserting 5 and 6: \", end=\"\")\n    print_list(last)\nclass Node:\n    def __init__(self, value):\n        self.data = value\n        self.next = None\n\n# Function to insert a node at a specific position in a circular linked list\ndef insertAtPosition(last, data, pos):\n    if last is None:\n        # If the list is empty\n        if pos != 1:\n            print(\"Invalid position!\")\n            return last\n        # Create a new node and make it point to itself\n        new_node = Node(data)\n        last = new_node\n        last.next = last\n        return last\n\n    # Create a new node with the given data\n    new_node = Node(data)\n\n    # curr will point to head initially\n    curr = last.next\n\n    if pos == 1:\n        # Insert at the beginning\n        new_node.next = curr\n        last.next = new_node\n        return last\n\n    # Traverse the list to find the insertion point\n    for i in range(1, pos - 1):\n        curr = curr.next\n\n        # If position is out of bounds\n        if curr == last.next:\n            print(\"Invalid position!\")\n            return last\n\n    # Insert the new node at the desired position\n    new_node.next = curr.next\n    curr.next = new_node\n\n    # Update last if the new node is inserted at the end\n    if curr == last:\n        last = new_node\n\n    return last\n\n# Function to print the circular linked list\ndef print_list(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\nif __name__ == \"__main__\":\n    # Create circular linked list: 2, 3, 4\n    first = Node(2)\n    first.next = Node(3)\n    first.next.next = Node(4)\n\n    last = first.next.next\n    last.next = first\n\n    print(\"Original list: \", end=\"\")\n    print_list(last)\n\n    # Insert elements at specific positions\n    data = 5\n    pos = 2\n    last = insertAtPosition(last, data, pos)\n    print(\"List after insertions: \", end=\"\")\n    print_list(last)"
                ]
            },
            {
                "Subtopic": "Deletion from a Circular Linked List",
                "Description": "Deletion involves removing a node from the linked list. The main difference is that we need to ensure the list remains circular after the deletion. We can delete a node in a circular linked list in three ways:",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\ndef deleteFirstNode(last):\n    if last is None:\n        # If the list is empty\n        print(\"List is empty\")\n        return None\n\n    head = last.next\n\n    if head == last:\n        # If there is only one node in the list\n        last = None\n    else:\n        # More than one node in the list\n        last.next = head.next\n\n    return last\n\ndef print_list(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\n# Create circular linked list: 2, 3, 4\nfirst = Node(2)\nfirst.next = Node(3)\nfirst.next.next = Node(4)\n\nlast = first.next.next\nlast.next = first\n\nprint(\"Original list: \", end=\"\")\nprint_list(last)\n\n# Delete the first node\nlast = deleteFirstNode(last)\n\nprint(\"List after deleting first node: \", end=\"\")\nprint_list(last)\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\ndef deleteSpecificNode(last, key):\n    if last is None:\n        # If the list is empty\n        print(\"List is empty, nothing to delete.\")\n        return None\n\n    curr = last.next\n    prev = last\n\n    # If the node to be deleted is the only node in the list\n    if curr == last and curr.data == key:\n        last = None\n        return last\n\n    # If the node to be deleted is the first node\n    if curr.data == key:\n        last.next = curr.next\n        return last\n\n    # Traverse the list to find the node to be deleted\n    while curr != last and curr.data != key:\n        prev = curr\n        curr = curr.next\n\n    # If the node to be deleted is found\n    if curr.data == key:\n        prev.next = curr.next\n        if curr == last:\n            last = prev\n    else:\n        # If the node to be deleted is not found\n        print(f\"Node with data {key} not found.\")\n\n    return last\n\ndef printList(last):\n    if last is None:\n        print(\"List is Empty\")\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\n# Create circular linked list: 2, 3, 4\nfirst = Node(2)\nfirst.next = Node(3)\nfirst.next.next = Node(4)\n\nlast = first.next.next\nlast.next = first\n\nprint(\"Original list: \", end=\"\")\nprintList(last)\n\n# Delete a specific node\nkey = 3\nlast = deleteSpecificNode(last, key)\n\nprint(f\"List after deleting node {key}: \", end=\"\")\nprintList(last)\nclass Node:\n    def __init__(self, data):\n        self.data = data\n        self.next = None\n\ndef deleteLastNode(last):\n    if last is None:\n        # If the list is empty\n        print(\"List is empty, nothing to delete.\")\n        return None\n\n    head = last.next\n\n    # If there is only one node in the list\n    if head == last:\n        last = None\n        return last\n\n    # Traverse the list to find the second last node\n    curr = head\n    while curr.next != last:\n        curr = curr.next\n\n    # Update the second last node's next pointer to point to head\n    curr.next = head\n    last = curr\n\n    return last\n\ndef printList(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\n# Create circular linked list: 2, 3, 4\nfirst = Node(2)\nfirst.next = Node(3)\nfirst.next.next = Node(4)\n\nlast = first.next.next\nlast.next = first\n\nprint(\"Original list: \", end=\"\")\nprintList(last)\n\n# Delete the last node\nlast = deleteLastNode(last)\n\nprint(\"List after deleting last node: \", end=\"\")\nprintList(last)"
                ]
            },
            {
                "Subtopic": "Searching in Circular Linked list",
                "Description": "Searching in a circular linked list is similar to searching in a regular linked list. We start at a given node and traverse the list until you either find the target value or return to the starting node. Since the list is circular, make sure to keep track of where you started to avoid an infinite loop.",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, value):\n        self.data = value\n        self.next = None\n\ndef search(last, key):\n    if last is None:\n        # If the list is empty\n        return False\n\n    head = last.next\n    curr = head\n\n    # Traverse the list to find the key\n    while curr != last:\n        if curr.data == key:\n            # Key found\n            return True\n        curr = curr.next\n\n    # Check the last node\n    if last.data == key:\n        # Key found\n        return True\n    # Key not found\n    return False\n\ndef print_list(last):\n    if last is None:\n        return\n\n    head = last.next\n    while True:\n        print(head.data, end=\" \")\n        head = head.next\n        if head == last.next:\n            break\n    print()\n\nif __name__ == \"__main__\":\n    # Create circular linked list: 2, 3, 4\n    first = Node(2)\n    first.next = Node(3)\n    first.next.next = Node(4)\n\n    last = first.next.next\n    last.next = first\n\n    print(\"Original list:\", end=\" \")\n    print_list(last)\n\n    # Search for a specific value\n    key = 3\n    found = search(last, key)\n    if found:\n        print(f\"Value {key} found in the list.\")\n    else:\n        print(f\"Value {key} not found in the list.\")"
                ]
            },
            {
                "Subtopic": "Advantages of Circular Linked Lists",
                "Description": "In circular linked list, the last node points to the first node. There are no null references, making traversal easier and reducing the chances of encountering null pointer exceptions. We can traverse the list from any node and return to it without needing to restart from the head, which is useful in applications requiring a circular iteration. Circular linked lists can easily implement circular queues, where the last element connects back to the first, allowing for efficient resource management. In a circular linked list, each node has a reference to the next node in the sequence. Although it doesn’t have a direct reference to the previous node like a doubly linked list, we can still find the previous node by traversing the list.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Circular Linked Lists",
                "Description": "Circular linked lists are more complex to implement than singly linked lists. Traversing a circular linked list without a clear stopping condition can lead to infinite loops if not handled carefully. Debugging can be more challenging due to the circular nature, as traditional methods of traversing linked lists may not apply.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Circular Linked Lists",
                "Description": "It is used for time-sharing among different users, typically through a  Round-Robin scheduling mechanism. In multiplayer games, a circular linked list can be used to switch between players. After the last player’s turn, the list cycles back to the first player. Circular linked lists are often used in buffering applications, such as streaming data, where data is continuously produced and consumed. In media players, circular linked lists can manage playlists, this allowing users to loop through songs continuously. Browsers use circular linked lists to manage the cache. This allows you to navigate back through your browsing history efficiently by pressing the BACK button. Related Article: Circular Linked List meaning in DSA Singly Linked List Tutorial Doubly Linked List Tutorial",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Stack",
        "Subtopics": [
            {
                "Subtopic": "Representation of Stack Data Structure:",
                "Description": "Stack follows LIFO (Last In First Out) Principle so the element which is pushed last is popped first.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Types of Stack:",
                "Description": "Fixed Size Stack   : As the name suggests, a fixed size stack has a fixed size and cannot grow or shrink dynamically. If the stack is full and an attempt is made to add an element to it, an overflow error occurs. If the stack is empty and an attempt is made to remove an element from it, an underflow error occurs.  \n Dynamic Size Stack   : A dynamic size stack can grow or shrink dynamically. When the stack is full, it automatically increases its size to accommodate the new element, and when the stack is empty, it decreases its size. This type of stack is implemented using a linked list, as it allows for easy resizing of the stack.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Basic Operations on Stack:",
                "Description": "In order to make manipulations in a stack, there are certain operations provided to us. push()   to insert an element into the stack  \n  pop()   to remove an element from the stack  \n  top()   Returns the top element of the stack.  \n  isEmpty()   returns true if stack is empty else false.  \n  isFull()   returns true if the stack is full else false. To implement stack, we need to maintain reference to the top item. Adds an item to the stack. If the stack is full, then it is said to be an   Overflow condition. Algorithm for Push Operation: Before pushing the element to the stack, we check if the stack is   full   .  \n If the stack is full   (top == capacity-1)   , then   Stack Overflows   and we cannot insert the element to the stack.  \n Otherwise, we increment the value of top by 1   (top = top + 1)   and the new value is inserted at   top position   .  \n The elements can be pushed into the stack till we reach the   capacity   of the stack. Removes an item from the stack. The items are popped in the reversed order in which they are pushed. If the stack is empty, then it is said to be an   Underflow  condition. Algorithm for Pop Operation: Before popping the element from the stack, we check if the stack is   empty   .  \n If the stack is empty (top == -1), then   Stack Underflows   and we cannot remove any element from the stack.  \n Otherwise, we store the value at top, decrement the value of top by 1   (top = top – 1)   and return the stored top value. Returns the top element of the stack. Algorithm for Top Operation: Before returning the top element from the stack, we check if the stack is empty.  \n If the stack is empty (top == -1), we simply print “Stack is empty”.  \n Otherwise, we return the element stored at   index = top   . Returns true if the stack is empty, else false. Algorithm for isEmpty Operation : Check for the value of   top   in stack.  \n If   (top == -1) , then the stack is   empty   so return   true   .  \n Otherwise, the stack is not empty so return   false   . Returns true if the stack is full, else false. Algorithm for isFull Operation: Check for the value of   top   in stack.  \n If   (top == capacity-1),   then the stack is   full   so return   true .  \n Otherwise, the stack is not full so return   false .",
                "Code Snippets": null
            },
            {
                "Subtopic": "Implementation of Stack",
                "Description": "The basic operations that can be performed on a stack include push, pop, and peek. There are two ways to implement a stack – Implementation of Stack using Array \n Implementation of Stack using Linked List",
                "Code Snippets": null
            },
            {
                "Subtopic": "Complexity Analysis of Operations on Stack Data Structure:",
                "Description": "Next Articles: Applications, Advantages and Disadvantages of Stack  \n Implement a stack using singly linked list  \n Basic Operations in Stack Data Structure with Implementations  \n Top 50 Problems on Stack Data Structure asked in SDE Interviews  \n Applications, Advantages and Disadvantages of Stack  \n Stack for Competitive Programming",
                "Code Snippets": null
            },
            {
                "Subtopic": "Implement Stack using Array:",
                "Description": "Step-by-step approach:",
                "Code Snippets": null
            },
            {
                "Subtopic": "Implement Stack Operations using Array:",
                "Description": "Here are the following operations of implement stack using array: Adds an item to the stack. If the stack is full, then it is said to be an  Overflow condition. Algorithm for Push Operation: Removes an item from the stack. The items are popped in the reversed order in which they are pushed. If the stack is empty, then it is said to be an  Underflow condition. Algorithm for Pop Operation:  Returns the top element of the stack. Algorithm for Top Operation: Returns true if the stack is empty, else false. Algorithm for isEmpty Operation  : Returns true if the stack is full, else false. Algorithm for isFull Operation: Below is the implementation of the above approach: Time Complexity : push : O(1) pop : O(1) peek : O(1) is_empty : O(1) is_full: O(1) Auxiliary Space : O(n), where n is the number of items in the stack.",
                "Code Snippets": [
                    "# Python program for implementation of stack \n\n# import maxsize from sys module \n# Used to return -infinite when stack is empty \nfrom sys import maxsize \n\n# Function to create a stack. It initializes size of stack as 0 \ndef createStack(): \n    stack = [] \n    return stack \n\n# Stack is empty when stack size is 0 \ndef isEmpty(stack): \n    return len(stack) == 0\n\n# Function to add an item to stack. It increases size by 1 \ndef push(stack, item): \n    stack.append(item) \n    print(item + \" pushed to stack \") \n    \n# Function to remove an item from stack. It decreases size by 1 \ndef pop(stack): \n    if (isEmpty(stack)): \n        return str(-maxsize -1) # return minus infinite \n    \n    return stack.pop() \n\n# Function to return the top from stack without removing it \ndef peek(stack): \n    if (isEmpty(stack)): \n        return str(-maxsize -1) # return minus infinite \n    return stack[len(stack) - 1] \n\n# Driver program to test above functions     \nstack = createStack() \npush(stack, str(10)) \npush(stack, str(20)) \npush(stack, str(30)) \nprint(pop(stack) + \" popped from stack\")"
                ]
            },
            {
                "Subtopic": "Advantages of Array Implementation:",
                "Description": "Easy to implement. Memory is saved as pointers are not involved.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Array Implementation:",
                "Description": "It is not dynamic i.e., it doesn’t grow and shrink depending on needs at runtime. [But in case of dynamic sized arrays like vector in C++, list in Python, ArrayList in Java, stacks can grow and shrink with array implementation as well]. The total size of the stack must be defined beforehand.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Stack Operations:",
                "Description": "push() :  Insert a new element into the stack i.e just insert a new element at the beginning of the linked list. pop() :  Return the top element of the Stack i.e simply delete the first element from the linked list. peek() :  Return the top element. display():  Print all elements in Stack.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Display Operation:",
                "Description": "Below is the implementation of the above operations Time Complexity:  O(1), for all push(), pop(), and peek(), as we are not performing any kind of traversal over the list. We perform all the operations through the current pointer only. Auxiliary Space:  O(N), where N is the size of the stack In this implementation, we define a Node class that represents a node in the linked list, and a Stack class that uses this node class to implement the stack. The head attribute of the Stack class points to the top of the stack (i.e., the first node in the linked list). To push an item onto the stack, we create a new node with the given item and set its next pointer to the current head of the stack. We then set the head of the stack to the new node, effectively making it the new top of the stack. To pop an item from the stack, we simply remove the first node from the linked list by setting the head of the stack to the next node in the list (i.e., the node pointed to by the next pointer of the current head). We return the data stored in the original head node, which is the item that was removed from the top of the stack. Dynamic memory allocation : The size of the stack can be increased or decreased dynamically by adding or removing nodes from the linked list, without the need to allocate a fixed amount of memory for the stack upfront. Efficient memory usage:  Since nodes in a singly linked list only have a next pointer and not a prev pointer, they use less memory than nodes in a doubly linked list. Easy implementation : Implementing a stack using a singly linked list is straightforward and can be done using just a few lines of code. Versatile : Singly linked lists can be used to implement other data structures such as queues, linked lists, and trees. In summary, implementing a stack using a singly linked list is a simple and efficient way to create a dynamic stack data structure in Python. Stacks are used in various real-world scenarios where a last-in, first-out (LIFO) data structure is required. Here are some examples of real-time applications of stacks: Function call stack : When a function is called in a program, the return address and all the function parameters are pushed onto the function call stack. The stack allows the function to execute and return to the caller function in the reverse order in which they were called. Undo/Redo operations:  In many applications, such as text editors, image editors, or web browsers, the undo and redo functionalities are implemented using a stack. Every time an action is performed, it is pushed onto the stack. When the user wants to undo the last action, the top element of the stack is popped and the action is reversed. Browser history:  Web browsers use stacks to keep track of the pages visited by the user. Every time a new page is visited, its URL is pushed onto the stack. When the user clicks the “Back” button, the last visited URL is popped from the stack and the user is directed to the previous page. Expression evaluation : Stacks are used in compilers and interpreters to evaluate expressions. When an expression is parsed, it is converted into postfix notation and pushed onto a stack. The postfix expression is then evaluated using the stack. Call stack in recursion:  When a recursive function is called, its call is pushed onto the stack. The function executes and calls itself, and each subsequent call is pushed onto the stack. When the recursion ends, the stack is popped, and the program returns to the previous function call. In summary, stacks are widely used in many applications where LIFO functionality is required, such as function calls, undo/redo operations, browser history, expression evaluation, and recursive function calls.",
                "Code Snippets": [
                    "# Java program to implement a stack using singly linked\n# list\n\n# Class representing a node in the linked list\nclass Node:\n    def __init__(self, new_data):\n        self.data = new_data\n        self.next = None\n\n# Class to implement stack using a singly linked list\nclass Stack:\n    def __init__(self):\n\n        # head of the linked list\n        self.head = None\n\n    # Function to check if the stack is empty\n    def is_empty(self):\n\n        # If head is None, the stack is empty\n        return self.head is None\n\n    # Function to push an element onto the stack\n    def push(self, new_data):\n\n        # Create a new node with given data\n        new_node = Node(new_data)\n\n        # Check if memory allocation for the new node failed\n        if not new_node:\n            print(\"\\nStack Overflow\")\n            return\n\n        # Link the new node to the current top node\n        new_node.next = self.head\n\n        # Update the top to the new node\n        self.head = new_node\n\n    # Function to remove the top element from the stack\n    def pop(self):\n\n        # Check for stack underflow\n        if self.is_empty():\n            print(\"\\nStack Underflow\")\n        else:\n\n            # Assign the current top to a temporary variable\n            temp = self.head\n\n            # Update the top to the next node\n            self.head = self.head.next\n\n            # Deallocate the memory of the old top node\n            del temp\n\n    # Function to return the top element of the stack\n    def peek(self):\n\n        # If stack is not empty, return the top element\n        if not self.is_empty():\n            return self.head.data\n        else:\n            print(\"\\nStack is empty\")\n            return float('-inf')\n\n\n# Creating a stack\nst = Stack()\n\n# Push elements onto the stack\nst.push(11)\nst.push(22)\nst.push(33)\nst.push(44)\n\n# Print top element of the stack\nprint(\"Top element is\", st.peek())\n\n# removing two elemements from the top\nprint(\"Removing two elements...\");\nst.pop()\nst.pop()\n\n# Print top element of the stack\nprint(\"Top element is\", st.peek())"
                ]
            }
        ]
    },
    {
        "Main Topic": "Insertion Sort Algorithm",
        "Subtopics": [
            {
                "Subtopic": "Insertion Sort Algorithm",
                "Description": "Insertion sort is a simple sorting algorithm that works by iteratively inserting each element of an unsorted list into its correct position in a sorted portion of the list. It is like sorting playing cards in your hands. You split the cards into two groups: the sorted cards and the unsorted cards. Then, you pick a card from the unsorted group and put it in the right place in the sorted group.\n\nWe start with second element of the array as first element in the array is assumed to be sorted.\nCompare second element with the first element and check if the second element is smaller then swap them.\nMove to the third element and compare it with the first two elements and put at its correct position\nRepeat until the entire array is sorted.",
                "Code Snippets": [
                    "# Python program for implementation of Insertion Sort\n\n# Function to sort array using insertion sort\ndef insertionSort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n\n        # Move elements of arr[0..i-1], that are\n        # greater than key, to one position ahead\n        # of their current position\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n\n# A utility function to print array of size n\ndef printArray(arr):\n    for i in range(len(arr)):\n        print(arr[i], end=\" \")\n    print()\n\n# Driver method\nif __name__ == \"__main__\":\n    arr = [12, 11, 13, 5, 6]\n    insertionSort(arr)\n    printArray(arr)"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Insertion Sort  :",
                "Description": "Best case:  O(n) , If the list is already sorted, where n is the number of elements in the list.   Average case:  O(n 2 ) , If the list is randomly ordered   Worst case:  O(n 2 ) , If the list is in reverse order Auxiliary Space:   O(1), Insertion sort requires   O(1)   additional space, making it a space-efficient sorting algorithm. Please refer  Complexity Analysis of Insertion Sort  for details.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages  of Insertion Sort:",
                "Description": "Simple and easy to implement.  Stable  sorting algorithm.   Efficient for small lists and nearly sorted lists.   Space-efficient as it is an in-place algorithm.  Adoptive. the  number of inversions  is directly proportional to number of swaps. For example, no swapping happens for a sorted array and it takes O(n) time only.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages  of Insertion Sort:",
                "Description": "Inefficient for large lists.   Not as efficient as other sorting algorithms (e.g., merge sort, quick sort) for most cases.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications  of Insertion Sort:",
                "Description": "Insertion sort is commonly used in situations where: The list is small or nearly sorted.  Simplicity and stability are important.  Used as a subroutine in  Bucket Sort Can be useful when array is already almost sorted (very few  inversions ) Since Insertion sort is suitable for small sized arrays, it is used in  Hybrid Sorting algorithms  along with other efficient algorithms like Quick Sort and Merge Sort.  When the subarray size becomes small, we switch to insertion sort in these recursive algorithms. For example  IntroSort  and  TimSort  use insertions sort.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Merge Sort",
        "Subtopics": [
            {
                "Subtopic": "Merge Sort",
                "Description": "Merge sort is a sorting algorithm that follows the divide-and-conquer approach. It works by recursively dividing the input array into smaller subarrays and sorting those subarrays then merging them back together to obtain the sorted array.\n\nIn simple terms, we can say that the process of merge sort is to divide the array into two halves, sort each half, and then merge the sorted halves back together. This process is repeated until the entire array is sorted.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How does Merge Sort work?",
                "Description": "Merge sort is a popular sorting algorithm known for its efficiency and stability. It follows the divide-and-conquer approach to sort a given array of elements.\nHere’s a step-by-step explanation of how merge sort works:\n\nDivide:  Divide the list or array recursively into two halves until it can no more be divided. \nConquer:  Each subarray is sorted individually using the merge sort algorithm. \nMerge:  The sorted subarrays are merged back together in sorted order. The process continues until all elements from both subarrays have been merged.",
                "Code Snippets": [
                    "def merge(arr, left, mid, right):\n    n1 = mid - left + 1\n    n2 = right - mid\n\n    # Create temp arrays\n    L = [0] * n1\n    R = [0] * n2\n\n    # Copy data to temp arrays L[] and R[]\n    for i in range(n1):\n        L[i] = arr[left + i]\n    for j in range(n2):\n        R[j] = arr[mid + 1 + j]\n\n    i = 0  # Initial index of first subarray\n    j = 0  # Initial index of second subarray\n    k = left  # Initial index of merged subarray\n\n    # Merge the temp arrays back\n    # into arr[left..right]\n    while i < n1 and j < n2:\n        if L[i] <= R[j]:\n            arr[k] = L[i]\n            i += 1\n        else:\n            arr[k] = R[j]\n            j += 1\n        k += 1\n\n    # Copy the remaining elements of L[],\n    # if there are any\n    while i < n1:\n        arr[k] = L[i]\n        i += 1\n        k += 1\n\n    # Copy the remaining elements of R[], \n    # if there are any\n    while j < n2:\n        arr[k] = R[j]\n        j += 1\n        k += 1\n\ndef merge_sort(arr, left, right):\n    if left < right:\n        mid = (left + right) // 2\n\n        merge_sort(arr, left, mid)\n        merge_sort(arr, mid + 1, right)\n        merge(arr, left, mid, right)\n\ndef print_list(arr):\n    for i in arr:\n        print(i, end=\" \")\n    print()\n\n# Driver code\nif __name__ == \"__main__\":\n    arr = [12, 11, 13, 5, 6, 7]\n    print(\"Given array is\")\n    print_list(arr)\n\n    merge_sort(arr, 0, len(arr) - 1)\n\n    print(\"\\nSorted array is\")\n    print_list(arr)"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Merge Sort",
                "Description": "Time Complexity:\nBest Case: O(n log n), When the array is already sorted or nearly sorted.\nAverage Case: O(n log n), When the array is randomly ordered.\nWorst Case: O(n log n), When the array is sorted in reverse order.\nAuxiliary Space: O(n), Additional space is required for the temporary array used during merging.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Merge Sort",
                "Description": "Sorting large datasets\nExternal sorting (when the dataset is too large to fit in memory)\nInversion counting\nMerge Sort and its variations are used in library methods of programming languages.\nIts variation TimSort is used in Python, Java Android and Swift. The main reason why it is preferred to sort non-primitive types is stability which is not there in QuickSort.\nArrays.sort in Java uses QuickSort while Collections.sort uses MergeSort.\nIt is a preferred algorithm for sorting Linked lists.\nIt can be easily parallelized as we can independently sort subarrays and then merge.\nThe merge function of merge sort to efficiently solve the problems like union and intersection of two sorted arrays.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages and Disadvantages of Merge Sort",
                "Description": "Advantages\n\nStability : Merge sort is a stable sorting algorithm, which means it maintains the relative order of equal elements in the input array.\nGuaranteed worst-case performance: Merge sort has a worst-case time complexity of O(N logN) , which means it performs well even on large datasets.\nSimple to implement: The divide-and-conquer approach is straightforward.\nNaturally Parallel : We independently merge subarrays that makes it suitable for parallel processing.\nDisadvantages\n\nSpace complexity: Merge sort requires additional memory to store the merged sub-arrays during the sorting process.\nNot in-place: Merge sort is not an in-place sorting algorithm, which means it requires additional memory to store the sorted data. This can be a disadvantage in applications where memory usage is a concern.\nMerge Sort is Slower than QuickSort in general as QuickSort is more cache friendly because it works in-place.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "QuickSort",
        "Subtopics": [
            {
                "Subtopic": "How does QuickSort Algorithm work?",
                "Description": "QuickSort works on the principle of  divide and conquer , breaking down the problem into smaller sub-problems. There are mainly three steps in the algorithm: Here’s a basic overview of how the QuickSort algorithm works. There are many different choices for picking pivots. Always pick the first (or last) element as a pivot . The below implementation picks the last element as pivot. The problem with this approach is it ends up in the worst case when array is already sorted.  Pick a random element as a pivot . This is a preferred approach because it does not have a pattern for which the worst case happens. Pick the median element is pivot. This is an ideal approach in terms of time complexity as  we can find median in linear time  and the partition function will always divide the input array into two halves. But it takes more time on average as median finding has high constants. The key process in  quickSort  is a  partition().  There are three common algorithms to partition. All these algorithms have O(n) time complexity.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Working of Partition Algorithm with Illustration",
                "Description": "Let us understand the working of partition algorithm with the help of the following example:",
                "Code Snippets": null
            },
            {
                "Subtopic": "Illustration of QuickSort Algorithm",
                "Description": "In the previous step, we looked at how the  partitioning  process rearranges the array based on the chosen  pivot . Next, we apply the same method recursively to the smaller sub-arrays on the  left  and  right  of the pivot. Each time, we select new pivots and partition the arrays again. This process continues until only one element is left, which is always sorted. Once every element is in its correct position, the entire array is sorted. Below image illustrates, how the recursive method calls for the smaller sub-arrays on the  left  and  right  of the  pivot : Quick Sort   is a crucial algorithm in the industry, but there are other sorting algorithms that may be more optimal in different cases.",
                "Code Snippets": [
                    "# Partition function\ndef partition(arr, low, high):\n    \n    # Choose the pivot\n    pivot = arr[high]\n    \n    # Index of smaller element and indicates \n    # the right position of pivot found so far\n    i = low - 1\n    \n    # Traverse arr[low..high] and move all smaller\n    # elements to the left side. Elements from low to \n    # i are smaller after every iteration\n    for j in range(low, high):\n        if arr[j] < pivot:\n            i += 1\n            swap(arr, i, j)\n    \n    # Move pivot after smaller elements and\n    # return its position\n    swap(arr, i + 1, high)\n    return i + 1\n\n# Swap function\ndef swap(arr, i, j):\n    arr[i], arr[j] = arr[j], arr[i]\n\n# The QuickSort function implementation\ndef quickSort(arr, low, high):\n    if low < high:\n        \n        # pi is the partition return index of pivot\n        pi = partition(arr, low, high)\n        \n        # Recursion calls for smaller elements\n        # and greater or equals elements\n        quickSort(arr, low, pi - 1)\n        quickSort(arr, pi + 1, high)\n\n# Main driver code\nif __name__ == \"__main__\":\n    arr = [10, 7, 8, 9, 1, 5]\n    n = len(arr)\n\n    quickSort(arr, 0, n - 1)\n    \n    for val in arr:\n        print(val, end=\" \")"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Quick Sort",
                "Description": "Time Complexity: Best Case:  (Ω(n log n)), Occurs when the pivot element divides the array into two equal halves. Average Case  (θ(n log n)), On average, the pivot divides the array into two parts, but not necessarily equal. Worst Case:  (O(n²)), Occurs when the smallest or largest element is always chosen as the pivot (e.g., sorted arrays). Auxiliary Space:  O(n),   due to   recursive call stack Please refer  Time and Space Complexity Analysis of Quick Sort  for more details.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages of Quick Sort",
                "Description": "It is a divide-and-conquer algorithm that makes it easier to solve problems.  It is efficient on large data sets.  It has a low overhead, as it only requires a small amount of memory to function.  It is Cache Friendly as we work on the same array to sort and do not copy data to any auxiliary array. Fastest general purpose algorithm for large data when stability is not required.  It is  tail recursive  and hence all the  tail call optimization  can be done.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Quick Sort",
                "Description": "It has a worst-case time complexity of O(n 2 ), which occurs when the pivot is chosen poorly.   It is not a good choice for small data sets.   It is not a stable sort, meaning that if two elements have the same key, their relative order will not be preserved in the sorted output in case of quick sort, because here we are swapping elements according to the pivot’s position (without considering their original positions).",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Quick Sort",
                "Description": "Efficient for sorting large datasets with O(n log n) average-case time complexity. Used in partitioning problems like finding the kth smallest element or dividing arrays by pivot. Integral to randomized algorithms, offering better performance than deterministic approaches. Applied in cryptography for generating random permutations and unpredictable encryption keys. Partitioning step can be parallelized for improved performance in multi-core or distributed systems. Important in theoretical computer science for analyzing average-case complexity and developing new techniques. Please refer  Application of Quicksort  for more details.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Bubble Sort",
        "Subtopics": [
            {
                "Subtopic": "How does Bubble Sort Work?",
                "Description": "Below is the implementation of the bubble sort. It can be optimized by stopping the algorithm if the inner loop didn’t cause any swap.",
                "Code Snippets": [
                    "# Optimized Python program for implementation of Bubble Sort\ndef bubbleSort(arr):\n    n = len(arr)\n    \n    # Traverse through all array elements\n    for i in range(n):\n        swapped = False\n\n        # Last i elements are already in place\n        for j in range(0, n-i-1):\n\n            # Traverse the array from 0 to n-i-1\n            # Swap if the element found is greater\n            # than the next element\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if (swapped == False):\n            break\n\n# Driver code to test above\nif __name__ == \"__main__\":\n    arr = [64, 34, 25, 12, 22, 11, 90]\n\n    bubbleSort(arr)\n\n    print(\"Sorted array:\")\n    for i in range(len(arr)):\n        print(\"%d\" % arr[i], end=\" \")"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Bubble Sort:",
                "Description": "Time Complexity:  O(n 2 ) Auxiliary Space:  O(1) Please refer  Complexity Analysis of Bubble Sort  for details.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages of Bubble Sort:",
                "Description": "Bubble sort is easy to understand and implement. It does not require any additional memory space. It is a stable sorting algorithm, meaning that elements with the same key value maintain their relative order in the sorted output.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Bubble Sort:",
                "Description": "Bubble sort has a time complexity of O(n 2 ) which makes it very slow for large data sets. Bubble sort has almost no or limited real world applications. It is mostly used in academics to teach different ways of sorting.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Bucket Sort",
        "Subtopics": [
            {
                "Subtopic": "Bucket Sort Algorithm:",
                "Description": "Create  n  empty buckets (Or lists) and do the following for every array element arr[i]. Insert arr[i] into bucket[n*array[i]] Sort individual buckets using insertion sort. Concatenate all sorted buckets.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How does Bucket Sort work?",
                "Description": "To apply bucket sort on the input array  [0.78, 0.17, 0.39, 0.26, 0.72, 0.94, 0.21, 0.12, 0.23, 0.68] , we follow these steps: Step 1:  Create an array of size 10, where each slot represents a bucket. Step 2:  Insert elements into the buckets from the input array based on their range. Inserting elements into the buckets: Take each element from the input array. Multiply the element by the size of the bucket array (10 in this case). For example, for element 0.23, we get 0.23 * 10 = 2.3. Convert the result to an integer, which gives us the bucket index. In this case, 2.3 is converted to the integer 2. Insert the element into the bucket corresponding to the calculated index. Repeat these steps for all elements in the input array. Step 3:  Sort the elements within each bucket. In this example, we use quicksort (or any stable sorting algorithm) to sort the elements within each bucket. Sorting the elements within each bucket: Apply a stable sorting algorithm (e.g., Bubble Sort, Merge Sort) to sort the elements within each bucket. The elements within each bucket are now sorted. Step 4:  Gather the elements from each bucket and put them back into the original array. Gathering elements from each bucket: Iterate through each bucket in order. Insert each individual element from the bucket into the original array. Once an element is copied, it is removed from the bucket. Repeat this process for all buckets until all elements have been gathered. Step 5:  The original array now contains the sorted elements. The final sorted array using bucket sort for the given input is [0.12, 0.17, 0.21, 0.23, 0.26, 0.39, 0.68, 0.72, 0.78, 0.94].",
                "Code Snippets": null
            },
            {
                "Subtopic": "Implementation of Bucket Sort Algorithm:",
                "Description": "Below is the implementation for the Bucket Sort:",
                "Code Snippets": [
                    "def insertion_sort(bucket):\n    for i in range(1, len(bucket)):\n        key = bucket[i]\n        j = i - 1\n        while j >= 0 and bucket[j] > key:\n            bucket[j + 1] = bucket[j]\n            j -= 1\n        bucket[j + 1] = key\n\ndef bucket_sort(arr):\n    n = len(arr)\n    buckets = [[] for _ in range(n)]\n\n    # Put array elements in different buckets\n    for num in arr:\n        bi = int(n * num)\n        buckets[bi].append(num)\n\n    # Sort individual buckets using insertion sort\n    for bucket in buckets:\n        insertion_sort(bucket)\n\n    # Concatenate all buckets into arr[]\n    index = 0\n    for bucket in buckets:\n        for num in bucket:\n            arr[index] = num\n            index += 1\n\narr = [0.897, 0.565, 0.656, 0.1234, 0.665, 0.3434]\nbucket_sort(arr)\nprint(\"Sorted array is:\")\nprint(\" \".join(map(str, arr)))"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Bucket Sort Algorithm:",
                "Description": "Worst Case Time Complexity:  O(n 2 )  The worst case happens when one bucket gets all the elements. In this case, we will be running insertion sort on all items which will make the time complexity as O(n 2 ).  We can reduce the worst case time complexity to O(n Log n) by using a O(n Log n) algorithm like Merge Sort or Heap Sort to sort the individual buckets, but that will improve the algorithm time for cases when buckets have small number of items as insertion sort works better for small arrays. Best Case Time Complexity :  O(n + k)  The best case happens when every bucket gets equal number of elements. In this case every call to insertion sort will take constant time as the number of items in every bucket would be constant (Assuming that k is linearly proportional to n). Auxiliary Space:  O(n+k)",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Binary Search",
        "Subtopics": [
            {
                "Subtopic": "What is Binary Search Algorithm?",
                "Description": "Binary search  is a search algorithm used to find the position of a target value within a  sorted  array. It works by repeatedly dividing the search interval in half until the target value is found or the interval is empty. The search interval is halved by comparing the target element with the middle value of the search space.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Conditions to apply Binary Search Algorithm in a Data Structure",
                "Description": "To apply Binary Search algorithm: The data structure must be sorted. Access to any element of the data structure should take constant time.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Binary Search Algorithm",
                "Description": "Below is the step-by-step algorithm for Binary Search: Divide the search space into two halves by  finding the middle index “mid” .  Compare the middle element of the search space with the  key .  If the  key  is found at middle element, the process is terminated. If the  key  is not found at middle element, choose which half will be used as the next search space. If the  key  is smaller than the middle element, then the  left  side is used for next search. If the  key  is larger than the middle element, then the  right  side is used for next search. This process is continued until the  key  is found or the total search space is exhausted.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How does Binary Search Algorithm work?",
                "Description": "To understand the working of binary search, consider the following illustration: Consider an array  arr[] = {2, 5, 8, 12, 16, 23, 38, 56, 72, 91} , and the  target = 23 .",
                "Code Snippets": null
            },
            {
                "Subtopic": "How to Implement Binary Search Algorithm?",
                "Description": "The  Binary Search Algorithm  can be implemented in the following two ways Iterative Binary Search Algorithm Recursive Binary Search Algorithm Given below are the pseudocodes for the approaches. Time Complexity:  O(log N) Auxiliary Space:  O(1)",
                "Code Snippets": [
                    "# Python3 code to implement iterative Binary\n# Search.\n\n\n# It returns location of x in given array arr\ndef binarySearch(arr, low, high, x):\n\n    while low <= high:\n\n        mid = low + (high - low) // 2\n\n        # Check if x is present at mid\n        if arr[mid] == x:\n            return mid\n\n        # If x is greater, ignore left half\n        elif arr[mid] < x:\n            low = mid + 1\n\n        # If x is smaller, ignore right half\n        else:\n            high = mid - 1\n\n    # If we reach here, then the element\n    # was not present\n    return -1\n\n\n# Driver Code\nif __name__ == '__main__':\n    arr = [2, 3, 4, 10, 40]\n    x = 10\n\n    # Function call\n    result = binarySearch(arr, 0, len(arr)-1, x)\n    if result != -1:\n        print(\"Element is present at index\", result)\n    else:\n        print(\"Element is not present in array\")\n# Python3 Program for recursive binary search.\n\n\n# Returns index of x in arr if present, else -1\ndef binarySearch(arr, low, high, x):\n\n    # Check base case\n    if high >= low:\n\n        mid = low + (high - low) // 2\n\n        # If element is present at the middle itself\n        if arr[mid] == x:\n            return mid\n\n        # If element is smaller than mid, then it\n        # can only be present in left subarray\n        elif arr[mid] > x:\n            return binarySearch(arr, low, mid-1, x)\n\n        # Else the element can only be present\n        # in right subarray\n        else:\n            return binarySearch(arr, mid + 1, high, x)\n\n    # Element is not present in the array\n    else:\n        return -1\n\n\n# Driver Code\nif __name__ == '__main__':\n    arr = [2, 3, 4, 10, 40]\n    x = 10\n    \n    # Function call\n    result = binarySearch(arr, 0, len(arr)-1, x)\n    \n    if result != -1:\n        print(\"Element is present at index\", result)\n    else:\n        print(\"Element is not present in array\")"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Binary Search Algorithm",
                "Description": "Time Complexity:   Best Case: O(1) Average Case: O(log N) Worst Case: O(log N) Auxiliary Space:  O(1), If the recursive call stack is considered then the auxiliary space will be O(logN).",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Binary Search Algorithm",
                "Description": "Binary search can be used as a building block for more complex algorithms used in machine learning, such as algorithms for training neural networks or finding the optimal hyperparameters for a model. It can be used for searching in computer graphics such as algorithms for ray tracing or texture mapping. It can be used for searching a database.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages of Binary Search",
                "Description": "Binary search is faster than linear search, especially for large arrays. More efficient than other searching algorithms with a similar time complexity, such as interpolation search or exponential search. Binary search is well-suited for searching large datasets that are stored in external memory, such as on a hard drive or in the cloud.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Binary Search",
                "Description": "The array should be sorted. Binary search requires that the data structure being searched be stored in contiguous memory locations.  Binary search requires that the elements of the array be comparable, meaning that they must be able to be ordered.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Binary Tree",
        "Subtopics": [
            {
                "Subtopic": "Representation of Binary Tree",
                "Description": "Each node in a Binary Tree has three parts: Data Pointer to the left child Pointer to the right child Syntax to declare a Node of Binary Tree in different languages:",
                "Code Snippets": [
                    "# A Python class that represents\n# an individual node in a Binary Tree\nclass Node:\n    def __init__(self, key):\n        self.left = None\n        self.right = None\n        self.val = key"
                ]
            },
            {
                "Subtopic": "Example for Creating a Binary Tree",
                "Description": "Here’s an example of creating a Binary Tree with four nodes (2, 3, 4, 5) In the above code, we have created four tree nodes  firstNode ,  secondNode ,  thirdNode  and  fourthNode  having values  2 ,  3 ,  4  and  5  respectively. After creating three nodes, we have connected these node to form the tree structure like mentioned in above image. Connect the  secondNode  to the left of  firstNode  by  firstNode->left = secondNode Connect the  thirdNode  to the right of  firstNode  by  firstNode->right = thirdNode Connect the  fourthNode  to the left of  secondNode  by  secondNode->left = fourthNode",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, d):\n        self.data = d\n        self.left = None\n        self.right = None\n\n# Initialize and allocate memory for tree nodes\nfirstNode = Node(2)\nsecondNode = Node(3)\nthirdNode = Node(4)\nfourthNode = Node(5)\n\n# Connect binary tree nodes\nfirstNode.left = secondNode\nfirstNode.right = thirdNode\nsecondNode.left = fourthNode"
                ]
            },
            {
                "Subtopic": "Terminologies in Binary Tree",
                "Description": "Nodes:  The fundamental part of a binary tree, where each node contains  data  and  link  to two child nodes. Root : The topmost node in a tree is known as the root node. It has no parent and serves as the starting point for all nodes in the tree. Parent Node : A node that has one or more child nodes. In a binary tree, each node can have at most two children. Child Node : A node that is a descendant of another node (its parent). Leaf Node : A node that does not have any children or both children are null. Internal Node : A node that has at least one child. This includes all nodes except the  root  and the  leaf  nodes. Depth of a Node : The number of edges from a specific node to the root node. The depth of the  root  node is zero. Height of a Binary Tree : The number of nodes from the deepest leaf node to the root node. The diagram below shows all these terms in a binary tree.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Properties of Binary Tree",
                "Description": "The maximum number of nodes at level  L  of a binary tree is  2 L The maximum number of nodes in a binary tree of height  H  is  2 H  – 1 Total number of leaf nodes in a binary tree = total number of nodes with 2 children + 1 In a Binary Tree with  N  nodes, the minimum possible height or the minimum number of levels is  Log 2 (N+1) A Binary Tree with  L  leaves has at least  | Log2L |+ 1  levels Please refer  Properties of Binary Tree  for more details.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Types of Binary Tree",
                "Description": "Binary Tree can be classified into multiples types based on multiple factors: On the basis of Number of Children Full Binary Tree Degenerate Binary Tree Skewed Binary Trees On the basis of Completion of Levels  Complete Binary Tree Perfect Binary Tree Balanced Binary Tree On the basis of Node Values: Binary Search Tree AVL Tree Red Black Tree B Tree B+ Tree Segment Tree",
                "Code Snippets": null
            },
            {
                "Subtopic": "Operations On Binary Tree",
                "Description": "Following is a list of common operations that can be performed on a binary tree: Traversal in Binary Tree involves visiting all the nodes of the binary tree. Tree Traversal algorithms can be classified broadly into two categories,  DFS  and  BFS : Depth-First Search (DFS) algorithms:  DFS explores as far down a branch as possible before backtracking. It is implemented using recursion. The main traversal methods in DFS for binary trees are: Preorder Traversal (current-left-right):   Visits the  node  first, then  left subtree , then  right subtree. Inorder Traversal (left-current-right):   Visits  left subtree , then the  node , then the  right subtree . Postorder Traversal (left-right-current):  Visits  left subtree , then  right subtree , then the  node . Breadth-First Search (BFS) algorithms:  BFS explores all nodes at the present depth before moving on to nodes at the next depth level. It is typically implemented using a queue.   BFS in a binary tree is commonly referred to as   Level Order Traversal . Below is the implementation of traversals algorithm in binary tree: Inserting elements means add a new node into the binary tree. As we know that there is no such ordering of elements in the binary tree, So we do not have to worry about the ordering of node in the binary tree. We would first creates a  root node  in case of empty tree. Then subsequent insertions involve iteratively searching for an empty place at each level of the tree. When an empty  left  or  right  child is found then  new node  is inserted there. By convention, insertion always starts with the  left  child node. Searching  for a value in a binary tree means looking through the tree to find a node that has that value. Since binary trees do not have a specific order like binary search trees, we typically use any traversal method to search. The most common methods are  depth-first search (DFS)  and  breadth-first search (BFS) . In  DFS , we start from the  root  and explore the depth nodes first. In BFS, we explore all the nodes at the present depth level before moving on to the nodes at the next level. We continue this process until we either find the node with the desired value or reach the end of the tree. If the tree is empty or the value isn’t found after exploring all possibilities, we conclude that the value does not exist in the tree. Here is the implementation of searching in a binary tree using Depth-First Search (DFS) Deleting a node from a binary tree means removing a specific node while keeping the tree’s structure. First, we need to find the node that want to delete by traversing through the tree using any traversal method. Then replace the node’s value with the value of the last node in the tree (found by traversing to the rightmost leaf), and then delete that last node. This way, the tree structure won’t be effected. And remember to check for special cases, like trying to delete from an empty tree, to avoid any issues. Note:  There is no specific rule of deletion but we always make sure that during deletion the binary tree proper should be preserved.",
                "Code Snippets": [
                    "class Node:\n    def __init__(self, data):\n        self.data = data\n        self.left = None\n        self.right = None\n\n# In-order DFS: Left, Root, Right\ndef in_order_dfs(node):\n    if node is None:\n        return\n    in_order_dfs(node.left)\n    print(node.data, end=' ')\n    in_order_dfs(node.right)\n\n# Pre-order DFS: Root, Left, Right\ndef pre_order_dfs(node):\n    if node is None:\n        return\n    print(node.data, end=' ')\n    pre_order_dfs(node.left)\n    pre_order_dfs(node.right)\n\n# Post-order DFS: Left, Right, Root\ndef post_order_dfs(node):\n    if node is None:\n        return\n    post_order_dfs(node.left)\n    post_order_dfs(node.right)\n    print(node.data, end=' ')\n\n# BFS: Level order traversal\ndef bfs(root):\n    if root is None:\n        return\n    queue = [root]\n    while queue:\n        node = queue.pop(0)\n        print(node.data, end=' ')\n        if node.left:\n            queue.append(node.left)\n        if node.right:\n            queue.append(node.right)\n\nif __name__ == \"__main__\":\n    # Creating the tree\n    root = Node(2)\n    root.left = Node(3)\n    root.right = Node(4)\n    root.left.left = Node(5)\n\n    print(\"In-order DFS: \", end='')\n    in_order_dfs(root)\n    print(\"\\nPre-order DFS: \", end='')\n    pre_order_dfs(root)\n    print(\"\\nPost-order DFS: \", end='')\n    post_order_dfs(root)\n    print(\"\\nLevel order: \", end='')\n    bfs(root)\nfrom collections import deque\n\nclass Node:\n    def __init__(self, d):\n        self.data = d\n        self.left = None\n        self.right = None\n\n# Function to insert a new node in the binary tree\ndef insert(root, key):\n    if root is None:\n        return Node(key)\n\n    # Create a queue for level order traversal\n    queue = deque([root])\n\n    while queue:\n        temp = queue.popleft()\n\n        # If left child is empty, insert the new node here\n        if temp.left is None:\n            temp.left = Node(key)\n            break\n        else:\n            queue.append(temp.left)\n\n        # If right child is empty, insert the new node here\n        if temp.right is None:\n            temp.right = Node(key)\n            break\n        else:\n            queue.append(temp.right)\n\n    return root\n\n# In-order traversal\ndef inorder(root):\n    if root is None:\n        return\n    inorder(root.left)\n    print(root.data, end=\" \")\n    inorder(root.right)\n\nif __name__ == \"__main__\":\n    root = Node(2)\n    root.left = Node(3)\n    root.right = Node(4)\n    root.left.left = Node(5)\n\n    print(\"Inorder traversal before insertion: \", end=\"\")\n    inorder(root)\n    print()\n\n    key = 6\n    root = insert(root, key)\n\n    print(\"Inorder traversal after insertion: \", end=\"\")\n    inorder(root)\n    print()\nclass Node:\n    def __init__(self, d):\n        self.data = d\n        self.left = None\n        self.right = None\n\n# Function to search for a value in the binary tree using DFS\ndef searchDFS(root, value):\n    # Base case: If the tree is empty or we've reached a leaf node\n    if root is None:\n        return False\n    # If the node's data is equal to the value we are searching for\n    if root.data == value:\n        return True\n    # Recursively search in the left and right subtrees\n    left_res = searchDFS(root.left, value)\n    right_res = searchDFS(root.right, value)\n\n    return left_res or right_res\n\nif __name__ == \"__main__\":\n    root = Node(2)\n    root.left = Node(3)\n    root.right = Node(4)\n    root.left.left = Node(5)\n    root.left.right = Node(6)\n\n    value = 6\n    if searchDFS(root, value):\n        print(f\"{value} is found in the binary tree\")\n    else:\n        print(f\"{value} is not found in the binary tree\")\nfrom collections import deque\n\nclass Node:\n    def __init__(self, d):\n        self.data = d\n        self.left = None\n        self.right = None\n\n# Function to delete a node from the binary tree\ndef deleteNode(root, val):\n    if root is None:\n        return None\n\n    # Use a queue to perform BFS\n    queue = deque([root])\n    target = None\n\n    # Find the target node\n    while queue:\n        curr = queue.popleft()\n\n        if curr.data == val:\n            target = curr\n            break\n        if curr.left:\n            queue.append(curr.left)\n        if curr.right:\n            queue.append(curr.right)\n\n    if target is None:\n        return root\n\n    # Find the deepest rightmost node and its parent\n    last_node = None\n    last_parent = None\n    queue = deque([(root, None)])\n\n    while queue:\n        curr, parent = queue.popleft()\n        last_node = curr\n        last_parent = parent\n\n        if curr.left:\n            queue.append((curr.left, curr))\n        if curr.right:\n            queue.append((curr.right, curr))\n\n    # Replace target's value with the last node's value\n    target.data = last_node.data\n\n    # Remove the last node\n    if last_parent:\n        if last_parent.left == last_node:\n            last_parent.left = None\n        else:\n            last_parent.right = None\n    else:\n        return None\n    return root\n\n# In-order traversal\ndef inorder(root):\n    if root is None:\n        return\n    inorder(root.left)\n    print(root.data, end=\" \")\n    inorder(root.right)\n\nif __name__ == \"__main__\":\n    root = Node(2)\n    root.left = Node(3)\n    root.right = Node(4)\n    root.left.left = Node(5)\n    root.left.right = Node(6)\n\n    print(\"Original tree (in-order): \", end=\"\")\n    inorder(root)\n    print()\n\n    val_to_del = 3\n    root = deleteNode(root, val_to_del)\n\n    print(f\"Tree after deleting {val_to_del} (in-order): \", end=\"\")\n    inorder(root)\n    print()"
                ]
            },
            {
                "Subtopic": "Auxiliary Operations On Binary Tree",
                "Description": "Finding the height of the tree Find level of a node in a Binary tree Finding the size of the entire tree",
                "Code Snippets": null
            },
            {
                "Subtopic": "Complexity Analysis of Binary Tree Operations",
                "Description": "Here’s the complexity analysis for specific binary tree operations: Note:  We can use  Morris Traversal   to traverse all the nodes of the binary tree in O(n) time complexity but with O(1) auxiliary space.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages of Binary Tree",
                "Description": "Efficient Search:  Binary Search Trees  (a variation of Binary Tree) are efficient when searching for a specific element, as each node has at most two child nodes when compared to linked list and arrays Memory Efficient:  Binary trees require lesser memory as compared to other tree data structures, therefore memory-efficient. Binary trees are relatively easy to implement and understand as each node has at most two children, left child and right child.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of Binary Tree",
                "Description": "Limited structure:  Binary trees are limited to two child nodes per node, which can limit their usefulness in certain applications. For example, if a tree requires more than two child nodes per node, a different tree structure may be more suitable. Unbalanced trees:  Unbalanced binary trees, where one subtree is significantly larger than the other, can lead to inefficient search operations. This can occur if the tree is not properly balanced or if data is inserted in a non-random order. Space inefficiency:  Binary trees can be space inefficient when compared to other data structures like arrays and linked list. This is because each node requires two child references or pointers, which can be a significant amount of memory overhead for large trees. Slow performance in worst-case scenarios:  In the worst-case scenario, a binary tree can become degenerate or skewed, meaning that each node has only one child. In this case, search operations in  Binary Search Tree  (a variation of Binary Tree) can degrade to O(n) time complexity, where n is the number of nodes in the tree.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Binary Tree",
                "Description": "Binary Tree can be used to  represent hierarchical data . Huffman Coding trees are used in  data compression algorithms . Priority Queue  is another application of binary tree that is used for searching maximum or minimum in O(1) time complexity. Useful for indexing segmented at the database is useful in storing cache in the system, Binary trees can be used to implement decision trees, a type of machine learning algorithm used for classification and regression analysis.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "DFS",
        "Subtopics": [
            {
                "Subtopic": "DFS Traversal of a Graph vs Tree:",
                "Description": "In the graph, there might be cycles and disconnectivity. Unlike the graph, the tree does not contain a cycle and are always connected. So DFS of a tree is relatively easier. We can simply begin from a node, then traverse its adjacent (or children) without caring about cycles. And if we begin from a single node (root), and traverse this way, it is guaranteed that we traverse the whole tree as there is no dis-connectivity, Examples: Below are the Tree traversals through DFS using recursion:",
                "Code Snippets": null
            },
            {
                "Subtopic": "1. Inorder Traversal (Practice):",
                "Description": "Follow the below steps to solve the problem: Traverse the left subtree, i.e., call Inorder(left-subtree) Visit the root Traverse the right subtree, i.e., call Inorder(right-subtree) Below is the implementation of the above algorithm: Time Complexity:  O(N) Auxiliary Space:  O(log N) In the case of binary search trees (BST), Inorder traversal gives nodes in non-decreasing order. To get nodes of BST in non-increasing order, a variation of Inorder traversal where Inorder traversal is reversed can be used",
                "Code Snippets": null
            },
            {
                "Subtopic": "2. Preorder Traversal (Practice):",
                "Description": "Follow the below steps to solve the problem: Visit the root Traverse the left subtree, i.e., call Preorder(left-subtree) Traverse the right subtree, i.e., call Preorder(right-subtree) Below is the implementation of the above algorithm: Time Complexity:  O(N) Auxiliary Space:  O(log N) Preorder traversal is used to create a copy of the tree. Preorder traversal is also used to get prefix expressions of an expression tree.",
                "Code Snippets": null
            },
            {
                "Subtopic": "3. Postorder Traversal (Practice):",
                "Description": "Follow the below steps to solve the problem: Traverse the left subtree, i.e., call Postorder(left-subtree) Traverse the right subtree, i.e., call Postorder(right-subtree) Visit the root Below is the implementation of the above algorithm: Time Complexity:  O(N) Auxiliary Space:  O(log N) Postorder traversal is used to delete the tree. Please see  the question for the deletion of the tree  for details. Postorder traversal is also useful to get the postfix expression of an expression tree",
                "Code Snippets": null
            },
            {
                "Subtopic": "Implementing all traversals using DFS:",
                "Description": "Time Complexity:  O(N) Auxiliary Space:  O(log N) Related Article: Please see  this post for Breadth First Traversal.",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Backtracking",
        "Subtopics": [
            {
                "Subtopic": "What is Backtracking?",
                "Description": "Backtracking is a problem-solving algorithmic technique that involves finding a solution incrementally by trying  different options  and  undoing  them if they lead to a  dead end . It is commonly used in situations where you need to explore multiple possibilities to solve a problem, like searching for a path in a maze or solving puzzles like Sudoku. When a dead end is reached, the algorithm backtracks to the previous decision point and explores a different path until a solution is found or all possibilities have been exhausted. Candidate:  A candidate is a potential choice or element that can be added to the current solution. \n Solution:  The solution is a valid and complete configuration that satisfies all problem constraints. \n Partial Solution:  A partial solution is an intermediate or incomplete configuration being constructed during the backtracking process. \n Decision Space:  The decision space is the set of all possible candidates or choices at each decision point. \n Decision Point:  A decision point is a specific step in the algorithm where a candidate is chosen and added to the partial solution. \n Feasible Solution:  A feasible solution is a partial or complete solution that adheres to all constraints. \n Dead End:  A dead end occurs when a partial solution cannot be extended without violating constraints. \n Backtrack:  Backtracking involves undoing previous decisions and returning to a prior decision point. \n Search Space:  The search space includes all possible combinations of candidates and choices. \n Optimal Solution:  In optimization problems, the optimal solution is the best possible solution.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Types of Backtracking Problems",
                "Description": "Problems associated with backtracking can be categorized into 3 categories: Decision Problems:  Here, we search for a feasible solution. \n Optimization Problems:  For this type, we search for the best solution. \n Enumeration Problems:  We find set of all possible feasible solutions to the problems of this type.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Determining Backtracking Problems:",
                "Description": "Generally every constraint satisfaction problem can be solved using backtracking but, Is it optimal to use backtracking every time? Turns out  NO , there are a vast number of problem that can be solved using  Greedy  or  Dynamic programming  in logarithmic or polynomial time complexity which is far better than exponential complexity of Backtracking. However many problems still exists that can only be solved using Backtracking.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Pseudocode for Backtracking",
                "Description": "The best way to implement backtracking is through recursion, and all backtracking code can be summarised as per the given Pseudocode:",
                "Code Snippets": null
            },
            {
                "Subtopic": "Complexity Analysis of Backtracking",
                "Description": "Since backtracking algorithm is purely brute force therefore in terms of time complexity, it performs very poorly. Generally backtracking can be seen having below mentioned time complexities: Exponential (O(K^N))  \n Factorial (O(N!)) These complexities are due to the fact that at each state we have multiple choices due to which the number of paths increases and sub-trees expand rapidly.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How Backtracking is different from Recursion?",
                "Description": "Recursion and Backtracking are related concepts in computer science and programming, but they are not the same thing. Let’s explore the key differences between them:",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of Backtracking",
                "Description": "Creating smart bots to play Board Games such as Chess. \n Solving mazes and puzzles such as N-Queen problem.  \n Network Routing and Congestion Control. \n Decryption \n Text Justification",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Graph",
        "Subtopics": [
            {
                "Subtopic": "Representations of Graph",
                "Description": "Here are the two most common ways to represent a graph : For simplicity, we are going to consider only  unweighted graphs  in this post.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Adjacency Matrix Representation",
                "Description": "An adjacency matrix is a way of representing a graph as a matrix of boolean (0’s and 1’s) Let’s assume there are  n  vertices in the graph So, create a 2D matrix  adjMat[n][n]  having dimension n x n. The below figure shows an undirected graph. Initially, the entire Matrix is ​​initialized to  0 . If there is an edge from source to destination, we insert  1  to both cases ( adjMat[destination]  and  adjMat [ destination])  because we can go either way. The below figure shows a directed graph. Initially, the entire Matrix is ​​initialized to  0 . If there is an edge from source to destination, we insert  1  for that particular  adjMat[destination] .",
                "Code Snippets": [
                    "def add_edge(mat, i, j):\n  \n    # Add an edge between two vertices\n    mat[i][j] = 1  # Graph is \n    mat[j][i] = 1  # Undirected\n\ndef display_matrix(mat):\n  \n    # Display the adjacency matrix\n    for row in mat:\n        print(\" \".join(map(str, row)))  \n\n# Main function to run the program\nif __name__ == \"__main__\":\n    V = 4  # Number of vertices\n    mat = [[0] * V for _ in range(V)]  \n\n    # Add edges to the graph\n    add_edge(mat, 0, 1)\n    add_edge(mat, 0, 2)\n    add_edge(mat, 1, 2)\n    add_edge(mat, 2, 3)\n\n    # Optionally, initialize matrix directly\n    \"\"\"\n    mat = [\n        [0, 1, 0, 0],\n        [1, 0, 1, 0],\n        [0, 1, 0, 1],\n        [0, 0, 1, 0]\n    ]\n    \"\"\"\n\n    # Display adjacency matrix\n    print(\"Adjacency Matrix:\")\n    display_matrix(mat)"
                ]
            },
            {
                "Subtopic": "Adjacency List Representation",
                "Description": "An array of Lists is used to store edges between two vertices. The size of array is equal to the number of  vertices (i.e, n) . Each index in this array represents a specific vertex in the graph. The entry at the index i of the array contains a linked list containing the vertices that are adjacent to vertex  i . Let’s assume there are  n  vertices in the graph So, create an  array of list  of size  n  as  adjList[n]. The below undirected graph has 3 vertices. So, an array of list will be created of size 3, where each indices represent the vertices. Now, vertex 0 has two neighbours (i.e, 1 and 2). So, insert vertex 1 and 2 at indices 0 of array. Similarly, For vertex 1, it has two neighbour (i.e, 2 and 0) So, insert vertices 2 and 0 at indices 1 of array. Similarly, for vertex 2, insert its neighbours in array of list. The below directed graph has 3 vertices. So, an array of list will be created of size 3, where each indices represent the vertices. Now, vertex 0 has no neighbours. For vertex 1, it has two neighbour (i.e, 0 and 2) So, insert vertices 0 and 2 at indices 1 of array. Similarly, for vertex 2, insert its neighbours in array of list.",
                "Code Snippets": [
                    "def add_edge(adj, i, j):\n    adj[i].append(j)\n    adj[j].append(i)  # Undirected\n\ndef display_adj_list(adj):\n    for i in range(len(adj)):\n        print(f\"{i}: \", end=\"\")\n        for j in adj[i]:\n            print(j, end=\" \")\n        print()\n\n# Create a graph with 4 vertices and no edges\nV = 4\nadj = [[] for _ in range(V)]\n\n# Now add edges one by one\nadd_edge(adj, 0, 1)\nadd_edge(adj, 0, 2)\nadd_edge(adj, 1, 2)\nadd_edge(adj, 2, 3)\n\nprint(\"Adjacency List Representation:\")\ndisplay_adj_list(adj)"
                ]
            },
            {
                "Subtopic": "BFS from a Given Source:",
                "Description": "The algorithm starts from a given source and explores all reachable vertices from the given source. It is similar to the   Breadth-First Traversal of a tree . Like tree, we begin with the given source (in tree, we begin with root) and traverse vertices level by level using a queue data structure.  The only catch here is that, unlike    trees,  graphs    may contain cycles, so we may come to the same node again. To avoid processing a node more than once, we use a  boolean   visited  array. Initialization:   Enqueue the given source vertex into a queue and mark it as visited. This algorithm ensures that all nodes in the graph are visited in a breadth-first manner, starting from the starting node. Time Complexity:   O(V+E), where V is the number of nodes and E is the number of edges.   Auxiliary Space:   O(V)",
                "Code Snippets": [
                    "from collections import deque\n\n# BFS from given source s\ndef bfs(adj, s):\n  \n    # Create a queue for BFS\n    q = deque()\n    \n    # Initially mark all the vertices as not visited\n    # When we push a vertex into the q, we mark it as \n    # visited\n    visited = [False] * len(adj);\n\n    # Mark the source node as visited and enqueue it\n    visited[s] = True\n    q.append(s)\n\n    # Iterate over the queue\n    while q:\n      \n        # Dequeue a vertex from queue and print it\n        curr = q.popleft()\n        print(curr, end=\" \")\n\n        # Get all adjacent vertices of the dequeued \n        # vertex. If an adjacent has not been visited, \n        # mark it visited and enqueue it\n        for x in adj[curr]:\n            if not visited[x]:\n                visited[x] = True\n                q.append(x)\n\n# Function to add an edge to the graph\ndef add_edge(adj, u, v):\n    adj[u].append(v)\n    adj[v].append(u)\n\n# Example usage\nif __name__ == \"__main__\":\n  \n    # Number of vertices in the graph\n    V = 5\n\n    # Adjacency list representation of the graph\n    adj = [[] for _ in range(V)]\n\n    # Add edges to the graph\n    add_edge(adj, 0, 1)\n    add_edge(adj, 0, 2)\n    add_edge(adj, 1, 3)\n    add_edge(adj, 1, 4)\n    add_edge(adj, 2, 4)\n\n    # Perform BFS traversal starting from vertex 0\n    print(\"BFS starting from 0: \")\n    bfs(adj, 0)"
                ]
            },
            {
                "Subtopic": "BFS of the whole Graph which Maybe Disconnected",
                "Description": "The above implementation takes a source as an input and prints only those vertices that are reachable from the source and  would not print all vertices in case of disconnected graph. Let us now  talk about the algorithm that prints all vertices without any source and  the graph maybe disconnected. The idea is simple, instead of calling BFS for a single vertex, we call the above implemented BFS for all all non-visited vertices one by one.",
                "Code Snippets": [
                    "from collections import deque\n\n# BFS from given source s\ndef bfs(adj, s, visited):\n  \n    q = deque() # Create a queue for BFS\n\n    # Mark the source node as visited and enqueue it\n    visited[s] = True\n    q.append(s)\n\n    # Iterate over the queue\n    while q:\n        curr = q.popleft() # Dequeue a vertex\n        print(curr, end=\" \")\n\n        # Get all adjacent vertices of curr\n        for x in adj[curr]:\n            if not visited[x]:\n                visited[x] = True # Mark as visited\n                q.append(x) # Enqueue it\n\n# Function to add an edge to the graph\ndef add_edge(adj, u, v):\n    adj[u].append(v)\n    adj[v].append(u) # Undirected graph\n\n# Perform BFS for the entire graph\ndef bfs_disconnected(adj):\n    visited = [False] * len(adj) # Not visited\n\n    for i in range(len(adj)):\n        if not visited[i]:\n            bfs(adj, i, visited)\n\n# Example usage\nV = 6 # Number of vertices\nadj = [[] for _ in range(V)] # Adjacency list\n\n# Add edges to the graph\nadd_edge(adj, 0, 1)\nadd_edge(adj, 0, 2)\nadd_edge(adj, 3, 4)\nadd_edge(adj, 4, 5)\n\n# Perform BFS traversal for the entire graph\nbfs_disconnected(adj)"
                ]
            },
            {
                "Subtopic": "Complexity Analysis of Breadth-First Search (BFS) Algorithm:",
                "Description": "BFS explores all the vertices and edges in the graph. In the worst case, it visits every vertex and edge once. Therefore, the time complexity of BFS is O(V + E), where V and E are the number of vertices and edges in the given graph. BFS uses a queue to keep track of the vertices that need to be visited. In the worst case, the queue can contain all the vertices in the graph. Therefore, the space complexity of BFS is O(V).",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of BFS in Graphs:",
                "Description": "BFS has various applications in graph theory and computer science, including: Shortest Path Finding:   BFS can be used to find the shortest path between two nodes in an unweighted graph. By keeping track of the parent of each node during the traversal, the shortest path can be reconstructed.   Cycle Detection:   BFS can be used to detect cycles in a graph. If a node is visited twice during the traversal, it indicates the presence of a cycle.   Connected Components:   BFS can be used to identify connected components in a graph. Each connected component is a set of nodes that can be reached from each other.   Topological Sorting:   BFS can be used to perform topological sorting on a directed acyclic graph (DAG). Topological sorting arranges the nodes in a linear order such that for any edge (u, v), u appears before v in the order.   Level Order Traversal of Binary Trees:   BFS can be used to perform a level order traversal of a binary tree. This traversal visits all nodes at the same level before moving to the next level.   Network Routing:   BFS can be used to find the shortest path between two nodes in a network, making it useful for routing data packets in network protocols.",
                "Code Snippets": null
            },
            {
                "Subtopic": "FAQs on Breadth First Search (BFS) for a Graph:",
                "Description": "Related Articles: Recent Articles on BFS   Depth First Traversal   Applications of Breadth First Traversal   Applications of Depth First Search   Time and Space Complexity of Breadth First Search (BFS)",
                "Code Snippets": null
            },
            {
                "Subtopic": "DFS from a Given Source of Undirected Graph:",
                "Description": "The algorithm starts from a given source and explores all reachable vertices from the given source. It is similar to  Preorder Tree Traversal  where we visit the root, then recur for its children. In a graph, there might be loops. So we use an extra visited array to make sure that we do not process a vertex again. Let us understand the working of   Depth First Search   with the help of the following illustration: for the  source as 0 . Below is the implementation of the above approach: Time complexity:  O(V + E), where V is the number of vertices and E is the number of edges in the graph. Auxiliary Space:  O(V + E), since an extra visited array of size V is required, And stack size for recursive calls to DFSRec function. Please refer  Complexity Analysis of Depth First Search:  for details.",
                "Code Snippets": [
                    "def dfs_rec(adj, visited, s):\n    # Mark the current vertex as visited\n    visited[s] = True\n\n    # Print the current vertex\n    print(s, end=\" \")\n\n    # Recursively visit all adjacent vertices\n    # that are not visited yet\n    for i in adj[s]:\n        if not visited[i]:\n            dfs_rec(adj, visited, i)\n\n\ndef dfs(adj, s):\n    visited = [False] * len(adj)\n    # Call the recursive DFS function\n    dfs_rec(adj, visited, s)\n\ndef add_edge(adj, s, t):\n    # Add edge from vertex s to t\n    adj[s].append(t)\n    # Due to undirected Graph\n    adj[t].append(s)\n    \nif __name__ == \"__main__\":\n    V = 5\n\n    # Create an adjacency list for the graph\n    adj = [[] for _ in range(V)]\n\n    # Define the edges of the graph\n    edges = [[1, 2], [1, 0], [2, 0], [2, 3], [2, 4]]\n\n    # Populate the adjacency list with edges\n    for e in edges:\n        add_edge(adj, e[0], e[1])\n\n    source = 1\n    print(\"DFS from source:\", source)\n    dfs(adj, source)"
                ]
            },
            {
                "Subtopic": "DFS for Complete Traversal of Disconnected Undirected Graph",
                "Description": "The above implementation takes a source as an input and prints only those vertices that are reachable from the source and would not print all vertices in case of disconnected graph. Let us now talk about the algorithm that prints all vertices without any source and the graph maybe disconnected. The idea is simple, instead of calling DFS for a single vertex, we call the above implemented DFS for all all non-visited vertices one by one. Time complexity:  O(V + E). Note that the time complexity is same here because we visit every vertex at most once and every edge is traversed at most once (in directed) and twice in undirected. Auxiliary Space:  O(V + E), since an extra visited array of size V is required, And stack size for recursive calls to DFSRec function. Related Articles: Depth First Search or DFS on Directed Graph Breadth First Search or BFS for a Graph",
                "Code Snippets": [
                    "class Graph:\n    def __init__(self, vertices):\n        # Adjacency list\n        self.adj = [[] for _ in range(vertices)]  \n\n    def add_edge(self, s, t):\n        self.adj[s].append(t)  \n        self.adj[t].append(s) \n\n    def dfs_rec(self, visited, s):\n        visited[s] = True \n        print(s, end=\" \") \n\n        # Recursively visit all adjacent vertices\n        # that are not visited yet\n        for i in self.adj[s]:\n            if not visited[i]:\n                self.dfs_rec(visited, i)\n\n    def dfs(self):\n        visited = [False] * len(self.adj) \n\n        # Loop through all vertices to handle disconnected\n        # graph\n        for i in range(len(self.adj)):\n            if not visited[i]:\n                  # Perform DFS from unvisited vertex\n                self.dfs_rec(visited, i)\n\n\nif __name__ == \"__main__\":\n    V = 6  # Number of vertices\n    graph = Graph(V)\n\n    # Define the edges of the graph\n    edges = [(1, 2), (2, 0), (0, 3), (4, 5)]\n\n    # Populate the adjacency list with edges\n    for edge in edges:\n        graph.add_edge(edge[0], edge[1])\n\n    print(\"Complete DFS of the graph:\")\n    graph.dfs()  # Perform DFS"
                ]
            },
            {
                "Subtopic": "Detect Cycle in a Directed Graph using  DFS:",
                "Description": "The problem can be solved based on the following idea: To keep track of vertices that are in recursion call stack, we use a boolean array where we use vertex number as an index. Whenever we begin recursive call for a vertex, we mark its entry as true and whenever the recursion call is about to end, we mark false. Illustration: Below is the graph showing how to detect cycle in a graph using DFS: Below is the implementation of the above approach: Time Complexity:  O(V + E), the Time Complexity of this method is the same as the time complexity of   DFS traversal   which is O(V+E).  Auxiliary Space:  O(V). To store the visited and recursion stack O(V) space is needed. In the below article, another O(V + E) method is discussed :  Detect Cycle in a direct graph using colors",
                "Code Snippets": [
                    "def is_cyc_util(adj, u, visited, rec_stack):\n  \n    if not visited[u]:\n      \n        # Mark the current node as visited\n        # and part of recursion stack\n        visited[u] = True\n        rec_stack[u] = True\n\n        # Recur for all the vertices \n        # adjacent to this vertex\n        for x in adj[u]:\n            if not visited[x] and is_cyc_util(adj, x, visited, rec_stack):\n                return True\n            elif rec_stack[x]:\n                return True\n\n    # Remove the vertex from recursion stack\n    rec_stack[u] = False\n    return False\n\ndef is_cyclic(adj, V):\n    visited = [False] * V\n    rec_stack = [False] * V\n\n    # Call the recursive helper function to\n    # detect cycle in different DFS trees\n    for i in range(V):\n        if not visited[i] and is_cyc_util(adj, i, visited, rec_stack):\n            return True\n\n    return False\n\n# Driver function\nif __name__ == \"__main__\":\n    V = 4\n    adj = [[] for _ in range(V)]\n\n    # Adding edges to the graph\n    adj[0].append(1)\n    adj[0].append(2)\n    adj[1].append(2)\n    adj[2].append(0)\n    adj[2].append(3)\n    adj[3].append(3)\n\n    # Function call\n    if is_cyclic(adj, V):\n        print(\"Contains Cycle\")\n    else:\n        print(\"No Cycle\")"
                ]
            },
            {
                "Subtopic": "Detect Cycle in a Directed Graph using  Topological Sorting:",
                "Description": "Below is the implementation of the above approach: Time Complexity:   O(V + E), the time complexity of this method is the same as the time complexity of BFS traversal which is O(V+E).  Auxiliary Space:   O(V)",
                "Code Snippets": [
                    "from collections import deque\n\n# Function to add an edge to the adjacency list\ndef add_edge(adj, u, v):\n    adj[u].append(v)\n\n# Function to detect cycle in a directed graph\ndef is_cyclic(V, adj):\n  \n    # Stores in-degree of each vertex\n    in_degree = [0] * V\n    \n    # Queue to store vertices with 0 in-degree\n    q = deque()\n    \n    visited = 0  # Count of visited vertices\n\n    # Calculate in-degree of each vertex\n    for u in range(V):\n        for v in adj[u]:\n            in_degree[v] += 1\n\n    # Enqueue vertices with 0 in-degree\n    for u in range(V):\n        if in_degree[u] == 0:\n            q.append(u)\n\n    # BFS traversal\n    while q:\n        u = q.popleft()\n        visited += 1\n\n        # Reduce in-degree of adjacent vertices\n        for v in adj[u]:\n            in_degree[v] -= 1\n            \n            # If in-degree becomes 0, enqueue it\n            if in_degree[v] == 0:\n                q.append(v)\n\n    # If not all vertices are visited, cycle\n    return visited != V\n\n# Driver function\nif __name__ == \"__main__\":\n    V = 6\n    adj = [[] for _ in range(V)]\n\n    # Adding edges to the graph\n    add_edge(adj, 0, 1)\n    add_edge(adj, 0, 2)\n    add_edge(adj, 1, 3)\n    add_edge(adj, 4, 1)\n    add_edge(adj, 4, 5)\n    add_edge(adj, 5, 3)\n\n    # Function call to check for cycles\n    if is_cyclic(V, adj):\n        print(\"Contains cycle\")\n    else:\n        print(\"No Cycle\")"
                ]
            },
            {
                "Subtopic": "1. Adjacency List for Directed graph:",
                "Description": "Consider an Directed and Unweighted graph  G  with 3  vertices  and  3 edges . For the graph G, the adjacency list would look like:",
                "Code Snippets": [
                    "# Function to add an edge between two vertices\ndef addEdge(adj, u, v):\n    adj[u].append(v)\n\n\ndef displayAdjList(adj):\n    for i in range(len(adj)):\n        print(f\"{i}: \", end=\"\")\n        for j in adj[i]:\n            print(f\"{j} \", end=\"\")\n        print()\n\n\ndef main():\n  \n    # Create a graph with 3 vertices and 3 edges\n    V = 3\n    adj = [[] for _ in range(V)]\n\n    # Now add edges one by one\n    addEdge(adj, 1, 0)\n    addEdge(adj, 1, 2)\n    addEdge(adj, 2, 0)\n\n    print(\"Adjacency List Representation:\")\n    displayAdjList(adj)\n\n\nif __name__ == \"__main__\":\n    main()"
                ]
            },
            {
                "Subtopic": "2. Adjacency List for Undirected graph:",
                "Description": "Consider an Undirected and Unweighted graph  G  with 3  vertices  and  3 edges . For the graph G, the adjacency list would look like:",
                "Code Snippets": [
                    "# Function to add an edge between two vertices\ndef addEdge(adj, u, v):\n    adj[u].append(v)\n    adj[v].append(u)\n\ndef displayAdjList(adj):\n    for i in range(len(adj)):\n        print(f\"{i}: \", end=\"\")\n        for j in adj[i]:\n            print(f\"{j} \", end=\"\")\n        print()\n\n\ndef main():\n  \n    # Create a graph with 3 vertices and 3 edges\n    V = 3\n    adj = [[] for _ in range(V)]\n\n    # Now add edges one by one\n    addEdge(adj, 1, 0)\n    addEdge(adj, 1, 2)\n    addEdge(adj, 2, 0)\n\n    print(\"Adjacency List Representation:\")\n    displayAdjList(adj)\n\n\nif __name__ == \"__main__\":\n    main()"
                ]
            },
            {
                "Subtopic": "3. Adjacency List for Directed and Weighted graph:",
                "Description": "Consider an Directed and Weighted graph  G  with 3  vertices  and  3 edges . For the graph G, the adjacency list would look like:",
                "Code Snippets": [
                    "# Function to add an edge between two vertices\ndef addEdge(adj, u, v, w):\n    adj[u].append((v, w))\n\ndef displayAdjList(adj):\n    for i in range(len(adj)):\n        print(f\"{i}: \", end=\"\")\n        for j in adj[i]:\n            print(f\"{{{j[0]}, {j[1]}}} \", end=\"\")\n        print()\n\ndef main():\n  \n    # Create a graph with 3 vertices and 3 edges\n    V = 3\n    adj = [[] for _ in range(V)]\n\n    # Now add edges one by one\n    addEdge(adj, 1, 0, 4)\n    addEdge(adj, 1, 2, 3)\n    addEdge(adj, 2, 0, 1)\n\n    print(\"Adjacency List Representation:\")\n    displayAdjList(adj)\n\nif __name__ == \"__main__\":\n    main()"
                ]
            },
            {
                "Subtopic": "4. Adjacency List for Undirected and Weighted graph:",
                "Description": "Consider an Undirected and Weighted graph  G  with 3  vertices  and  3 edges . For the graph G, the adjacency list would look like:",
                "Code Snippets": [
                    "# Function to add an edge between two vertices\ndef addEdge(adj, u, v, w):\n    adj[u].append((v, w))\n    adj[v].append((u, w))\n\ndef displayAdjList(adj):\n    for i in range(len(adj)):\n        print(f\"{i}: \", end=\"\")\n        for j in adj[i]:\n            print(f\"{{{j[0]}, {j[1]}}} \", end=\"\")\n        print()\n\ndef main():\n  \n    # Create a graph with 3 vertices and 3 edges\n    V = 3\n    adj = [[] for _ in range(V)]\n\n    # Now add edges one by one\n    addEdge(adj, 1, 0, 4)\n    addEdge(adj, 1, 2, 3)\n    addEdge(adj, 2, 0, 1)\n\n    print(\"Adjacency List Representation:\")\n    displayAdjList(adj)\n\nif __name__ == \"__main__\":\n    main()"
                ]
            },
            {
                "Subtopic": "Characteristics of the Adjacency List:",
                "Description": "An adjacency list representation uses a list of lists. We store all adjacent of every node together. The size of the list is determined by the number of vertices in the graph. All adjacent of a vertex are easily available. To find all adjacent, we need only O(n) time where is the number of adjacent vertices.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Applications of the Adjacency List:",
                "Description": "Graph algorithms : Many graph algorithms like  Dijkstra’s algorithm ,  Breadth First Search , and  Depth First Search  perform faster for adjacency lists to represent graphs. Adjacency List representation is the most commonly used representation of graph as it allows easy traversal of all edges.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Advantages of using an Adjacency list:",
                "Description": "An adjacency list is simple and easy to understand. Requires less space compared to adjacency matrix for sparse graphs. Easy to traverse through all edges of a graph. Adding an vertex is easier compared to adjacency matrix representation. Most of the graph algorithms are implemented faster with this representation. For example, BFS and DFS implementations take OIV x V) time, but with Adjacency List representation, we get these in linear time. Similarly Prim’s and Dijskstra’s algorithms are implemented faster with Adjacency List representation.",
                "Code Snippets": null
            },
            {
                "Subtopic": "Disadvantages of using an Adjacency list:",
                "Description": "Checking if there is an edge between two vertices is costly as we have traverse the adjacency list. Not suitable for dense graphs. Adjacency Matrix meaning and definition in DSA Add and Remove Edge in Adjacency List representation of a Graph Convert Adjacency Matrix to Adjacency List representation of Graph Convert Adjacency List to Adjacency Matrix representation of a Graph Comparison between Adjacency List and Adjacency Matrix representation of Graph",
                "Code Snippets": null
            }
        ]
    },
    {
        "Main Topic": "Rabin-Karp Algorithm",
        "Subtopics": [
            {
                "Subtopic": "Rabin-Karp Algorithm:",
                "Description": "In the  Naive String Matching  algorithm, we check whether every substring of the text of the pattern’s size is equal to the pattern or not one by one.",
                "Code Snippets": null
            },
            {
                "Subtopic": "How is Hash Value calculated in Rabin-Karp?",
                "Description": "Hash value  is used to efficiently check for potential matches between a  pattern  and substrings of a larger  text . The hash value is calculated using a  rolling hash function , which allows you to update the hash value for a new substring by efficiently removing the contribution of the old character and adding the contribution of the new character. This makes it possible to slide the pattern over the  text  and calculate the hash value for each substring without recalculating the entire hash from scratch. Here’s how the hash value is typically calculated in Rabin-Karp: Step 1:  Choose a suitable  base  and a  modulus : Select a prime number ‘ p ‘ as the modulus. This choice helps avoid overflow issues and ensures a good distribution of hash values. Choose a base ‘ b ‘ (usually a prime number as well), which is often the size of the character set (e.g., 256 for ASCII characters). Step 2:  Initialize the hash value: Set an initial hash value ‘ hash ‘ to  0 . Step 3:  Calculate the initial hash value for the  pattern : Iterate over each character in the  pattern  from  left  to  right . For each character  ‘c’  at position  ‘i’ , calculate its contribution to the hash value as  ‘c * (b pattern_length – i – 1 ) % p’  and add it to ‘ hash ‘. This gives you the hash value for the entire  pattern . Step 4:  Slide the pattern over the  text : Start by calculating the hash value for the first substring of the  text  that is the same length as the  pattern . Step 5:  Update the hash value for each subsequent substring: To slide the  pattern  one position to the right, you remove the contribution of the leftmost character and add the contribution of the new character on the right. The formula for updating the hash value when moving from position  ‘i’  to  ‘i+1’  is: Step 6:  Compare hash values: When the hash value of a substring in the  text  matches the hash value of the  pattern , it’s a  potential match . If the hash values match, we should perform a character-by-character comparison to confirm the match, as  hash collisions  can occur. Below is the Illustration of above algorithm:  Step-by-step approach: Initially calculate the hash value of the pattern. Start iterating from the starting of the string: Calculate the hash value of the current substring having length  m . If the hash value of the current substring and the pattern are same check if the substring is same as the pattern. If they are same, store the starting index as a valid answer. Otherwise, continue for the next substrings. Return the starting indices as the required answer. Below is the implementation of the above approach: Time Complexity: The average and best-case running time of the Rabin-Karp algorithm is O(n+m), but its worst-case time is O(nm). The worst case of the Rabin-Karp algorithm occurs when all characters of pattern and text are the same as the hash values of all the substrings of T[] match with the hash value of P[]. Auxiliary Space:  O(1)",
                "Code Snippets": [
                    "# Following program is the python implementation of\n# Rabin Karp Algorithm given in CLRS book\n\n# d is the number of characters in the input alphabet\nd = 256\n\n# pat  -> pattern\n# txt  -> text\n# q    -> A prime number\n\n\ndef search(pat, txt, q):\n    M = len(pat)\n    N = len(txt)\n    i = 0\n    j = 0\n    p = 0    # hash value for pattern\n    t = 0    # hash value for txt\n    h = 1\n\n    # The value of h would be \"pow(d, M-1)%q\"\n    for i in range(M-1):\n        h = (h*d) % q\n\n    # Calculate the hash value of pattern and first window\n    # of text\n    for i in range(M):\n        p = (d*p + ord(pat[i])) % q\n        t = (d*t + ord(txt[i])) % q\n\n    # Slide the pattern over text one by one\n    for i in range(N-M+1):\n        # Check the hash values of current window of text and\n        # pattern if the hash values match then only check\n        # for characters one by one\n        if p == t:\n            # Check for characters one by one\n            for j in range(M):\n                if txt[i+j] != pat[j]:\n                    break\n                else:\n                    j += 1\n\n            # if p == t and pat[0...M-1] = txt[i, i+1, ...i+M-1]\n            if j == M:\n                print(\"Pattern found at index \" + str(i))\n\n        # Calculate hash value for next window of text: Remove\n        # leading digit, add trailing digit\n        if i < N-M:\n            t = (d*(t-ord(txt[i])*h) + ord(txt[i+M])) % q\n\n            # We might get negative values of t, converting it to\n            # positive\n            if t < 0:\n                t = t+q\n\n\n# Driver Code\nif __name__ == '__main__':\n    txt = \"GEEKS FOR GEEKS\"\n    pat = \"GEEK\"\n\n    # A prime number\n    q = 101\n\n    # Function Call\n    search(pat, txt, q)\n\n# This code is contributed by Bhavya Jain"
                ]
            },
            {
                "Subtopic": "Limitations of Rabin-Karp Algorithm",
                "Description": "Spurious Hit:  When the hash value of the pattern matches with the hash value of a window of the text but the window is not the actual pattern then it is called a  spurious hit . Spurious hit increases the time complexity of the algorithm. In order to minimize spurious hit, we use good  hash function . It greatly reduces the spurious hit. Related Posts:   Searching for Patterns | Set 1 (Naive Pattern Searching)   Searching for Patterns | Set 2 (KMP Algorithm)",
                "Code Snippets": null
            }
        ]
    }
]