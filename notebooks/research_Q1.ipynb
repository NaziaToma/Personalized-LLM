{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "365ab701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RQ1: Step 1 - Setting up a prior belief based on targeted question\n",
    "\n",
    "#targeted questions\n",
    "question1 = \"Do you prefer direct next steps or detailed discussion first?\"\n",
    "question1_options=[\"1. direct steps\", \"2. detailed discussion\"]\n",
    "\n",
    "question2= \"Would you prefer a checklist or conversation?\"\n",
    "question2_options=[\"1. checklist\", \"2. conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ef79fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you prefer direct next steps or detailed discussion first?\n",
      "1. direct steps\n",
      "2. detailed discussion\n",
      "Would you prefer a checklist or conversation?\n",
      "1. checklist\n",
      "2. conversation\n",
      "['1. direct steps', '1. checklist']\n"
     ]
    }
   ],
   "source": [
    "#Function for asking question\n",
    "\n",
    "def ask_question(question, answer):\n",
    "    #question 1\n",
    "    print(question)\n",
    "    for i in range(len(answer)):\n",
    "        print(answer[i])\n",
    "    choice= input('Enter your preferred answer number') #change it to better UI based button later\n",
    "    return answer[int(choice)-1]\n",
    "\n",
    "\n",
    "choice_result = []\n",
    "\n",
    "choice_result.append(ask_question(question1, question1_options))\n",
    "choice_result.append(ask_question(question2, question2_options))\n",
    "\n",
    "print(choice_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817a7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preferred_conversation_style': 'action_based', 'ph1': None, 'ph2': None}\n"
     ]
    }
   ],
   "source": [
    "#creating user profile based on user's answer\n",
    "\n",
    "user_profile = {\"preferred_conversation_style\": None,\n",
    "                \"ph1\": None,\n",
    "                \"ph2\": None}\n",
    "\n",
    "#take 2 answers and based on that assign key-value pair for user profile--think\n",
    "\n",
    "\n",
    "if choice_result[0]==\"1. direct steps\" and choice_result[1] == \"1. checklist\":\n",
    "    user_profile[\"preferred_conversation_style\"] = \"action_based\"\n",
    "elif choice_result[0]==\"2. detailed discussion\" and choice_result[1] == \"2. conversation\":\n",
    "    user_profile[\"preferred_conversation_style\"] = \"relationship_based\"\n",
    "else:\n",
    "    user_profile[\"preferred_conversation_style\"] = \"mixed\"\n",
    "        \n",
    "print(user_profile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7e4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preferred_conversation_style': 'action_based', 'ph1': 0.9, 'ph2': 0.1}\n"
     ]
    }
   ],
   "source": [
    "#assign prior belief value - hardcoded, might change with an LLM prompt layer (step 1 completed)\n",
    "\n",
    "if user_profile[\"preferred_conversation_style\"] == \"action_based\":\n",
    "    user_profile[\"ph1\"] = .90\n",
    "    user_profile[\"ph2\"] = .10\n",
    "elif user_profile[\"preferred_conversation_style\"] == \"relationship_based\":\n",
    "    user_profile[\"ph1\"] = .10\n",
    "    user_profile[\"ph2\"] = .90\n",
    "else:\n",
    "    user_profile[\"ph1\"]=.60\n",
    "    user_profile[\"ph2\"]=.40\n",
    "    \n",
    "print(user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6888d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RQ1: Step 2 - Observing behavior with LLM prompt and treating each message like evidance P(E)\n",
    "\n",
    "user_input = input(\"Hi there! What you want to chat about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e0739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\GitHub Repos\\Personalized-LLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08962c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdb6e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [02:18<00:00, 69.35s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4491967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a conversation analyst. Estimate how likely it is that the user is Action Based or Relationship Based. Give me peh1 and peh2 values where peh1 is probability user would say that if they are action based and peh2 is probability user would say that if they are relationship based. Here is the user input: About problem solving, I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I think it is important to be able to think outside the box. I\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(f'''You are a conversation analyst. Estimate how likely it is that the user is Action Based or Relationship Based. Give me peh1 and peh2 values where peh1 is probability user would say that if they are action based and peh2 is probability user would say that if they are relationship based. Here is the user input: {user_input}''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "# add prompt engioneering to close the loop \n",
    "\n",
    "\n",
    "# MOVE inputs to model's device\n",
    "device = model.device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()} \n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
